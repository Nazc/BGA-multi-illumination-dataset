{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18474131",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd816edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchcam\n",
      "  Using cached torchcam-0.4.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in c:\\users\\nalex\\onedrive\\documentos\\phd\\vsprojects\\bga_defectclass\\venv\\lib\\site-packages (from torchcam) (2.7.1+cu118)\n",
      "Collecting numpy<2.0.0,>=1.17.2 (from torchcam)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + C:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Scripts\\python.exe C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\\vendored-meson\\meson\\meson.py setup C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\\.mesonpy-z7wnn259 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\\.mesonpy-z7wnn259\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\n",
      "      Build dir: C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\\.mesonpy-z7wnn259\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `cl /?` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `cc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gcc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `clang --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\nalex\\AppData\\Local\\Temp\\pip-install-fzulp1g4\\numpy_e67756dcafb04982b844b724a627ecdd\\.mesonpy-z7wnn259\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# !pip install Pillow\n",
    "# !pip install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install pandas==2.3.2\n",
    "# !pip install scikit-learn==1.7.1\n",
    "# !pip install matplotlib==3.10.5\n",
    "# !pip install shap\n",
    "# !pip install lime\n",
    "# !pip install torchcam \n",
    "# !pip install numpy\n",
    "# !pip install numpy==1.25.2\n",
    "# !pip install torchcam\n",
    "# !pip install torchcam -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51894662",
   "metadata": {},
   "source": [
    "# Augmentation and Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd87fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: ..\\data\\imgShiny\\original\n",
      "  absence: 80 imágenes renombradas.\n",
      "  presence-defect: 203 imágenes renombradas.\n",
      "  presence-good: 51 imágenes renombradas.\n",
      "Procesando: ..\\data\\png4ch\\original\n",
      "  absence: 80 imágenes renombradas.\n",
      "  presence-defect: 206 imágenes renombradas.\n",
      "  presence-good: 51 imágenes renombradas.\n",
      "Renombrado completado.\n"
     ]
    }
   ],
   "source": [
    "# Script to rename images in each 'original' subfolder by category\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Datasets\n",
    "roots = [\n",
    "    Path(\"../data/imgShiny/original\"),\n",
    "    Path(\"../data/png4ch/original\")\n",
    " ]\n",
    "\n",
    "for root in roots:\n",
    "    print(f\"Processing: {root}\")\n",
    "    # Iterate over each category subfolder\n",
    "    for category in os.listdir(root):\n",
    "        cat_dir = root / category\n",
    "        if not cat_dir.is_dir():\n",
    "            continue\n",
    "        # List all image files in the category folder\n",
    "        images = sorted([p for p in cat_dir.iterdir() if p.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']])\n",
    "        for idx, img_path in enumerate(images, 1):\n",
    "            # Build new filename: category_001.ext, category_002.ext, ...\n",
    "            new_name = f\"{category}_{idx:03d}{img_path.suffix.lower()}\"\n",
    "            new_path = cat_dir / new_name\n",
    "            # Rename if the filename is different\n",
    "            if img_path != new_path:\n",
    "                os.rename(img_path, new_path)\n",
    "        print(f\"  {category}: {len(images)} images renamed.\")\n",
    "print(\"Renaming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(original_root, augmented_root, target_count):\n",
    "    \"\"\"\n",
    "    Performs image augmentation in the folders of original_root and saves to augmented_root.\n",
    "    - original_root: input path with subfolders per class.\n",
    "    - augmented_root: output path where augmented images will be saved.\n",
    "    - target_count: total number of images per class after augmentation.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from PIL import Image\n",
    "    import random\n",
    "    from torchvision import transforms\n",
    "\n",
    "    original_root = Path(original_root)\n",
    "    augmented_root = Path(augmented_root)\n",
    "\n",
    "    # Find categories\n",
    "    categories = [d for d in os.listdir(original_root) if (original_root / d).is_dir()]\n",
    "\n",
    "    for cat in categories:\n",
    "        orig_dir = original_root / cat\n",
    "        aug_dir = augmented_root / cat\n",
    "        aug_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # List all valid image files\n",
    "        images = [p for p in orig_dir.glob('*') if p.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']]\n",
    "        n_orig = len(images)\n",
    "\n",
    "        # Detect mode of the first valid image (RGB or RGBA)\n",
    "        mode = None\n",
    "        for img_path in images:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                if img.mode in ['RGB', 'RGBA']:\n",
    "                    mode = img.mode\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[WARNING] Image ignored due to unsupported mode: {img_path} (mode: {img.mode})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Could not open {img_path}: {e}\")\n",
    "\n",
    "        if mode is None:\n",
    "            print(f\"[WARNING] No RGB or RGBA images found in category {cat}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Define augmentation pipeline according to image mode\n",
    "        if mode == 'RGBA':\n",
    "            augment = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
    "            ])\n",
    "        else:\n",
    "            augment = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
    "            ])\n",
    "\n",
    "        # Copy valid originals\n",
    "        valid_images = []\n",
    "        for idx, img_path in enumerate(images):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                if img.mode == mode:\n",
    "                    img.save(aug_dir / f'orig_{idx+1:04d}.png')\n",
    "                    valid_images.append(img_path)\n",
    "                else:\n",
    "                    print(f\"[WARNING] Image ignored due to different mode: {img_path} (mode: {img.mode})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Could not open {img_path}: {e}\")\n",
    "\n",
    "        n_valid = len(valid_images)\n",
    "\n",
    "        # Augment until target_count\n",
    "        extra_needed = target_count - n_valid\n",
    "        if extra_needed > 0 and valid_images:\n",
    "            for i in range(extra_needed):\n",
    "                src_path = random.choice(valid_images)\n",
    "                img = Image.open(src_path).convert(mode)\n",
    "                aug_img = augment(img)\n",
    "                aug_img.save(aug_dir / f'aug_{i+1:04d}.png')\n",
    "\n",
    "        print(f'Category: {cat} | Valid originals: {n_valid} | Total saved: {max(target_count, n_valid)} in {aug_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d07a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 creado en ..\\data\\png4ch\\224px\\orFold1\n",
      "Fold 2 creado en ..\\data\\png4ch\\224px\\orFold2\n",
      "Fold 2 creado en ..\\data\\png4ch\\224px\\orFold2\n",
      "Fold 3 creado en ..\\data\\png4ch\\224px\\orFold3\n",
      "Fold 3 creado en ..\\data\\png4ch\\224px\\orFold3\n",
      "Fold 4 creado en ..\\data\\png4ch\\224px\\orFold4\n",
      "Fold 4 creado en ..\\data\\png4ch\\224px\\orFold4\n",
      "Fold 5 creado en ..\\data\\png4ch\\224px\\orFold5\n",
      "Fold 5 creado en ..\\data\\png4ch\\224px\\orFold5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Params\n",
    "# original_root = Path(\"../data/imgShiny/original\")\n",
    "# output_root = Path(\"../data/imgShiny/224px\")\n",
    "original_root = Path(\"../data/png4ch/original\")\n",
    "output_root = Path(\"../data/png4ch/224px\")\n",
    "n_folds = 5\n",
    "split_ratio = [0.7, 0.15, 0.15]  # train, val, test\n",
    "\n",
    "# Get categories\n",
    "categories = [d for d in os.listdir(original_root) if (original_root / d).is_dir()]\n",
    "\n",
    "for fold in range(1, n_folds+1):\n",
    "    fold_dir = output_root / f\"orFold{fold}\"\n",
    "    for split, ratio in zip(['train', 'val', 'test'], split_ratio):\n",
    "        for cat in categories:\n",
    "            split_dir = fold_dir / split / cat\n",
    "            split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # For each class, shuffle and split images\n",
    "    for cat in categories:\n",
    "        img_dir = original_root / cat\n",
    "        images = [p for p in img_dir.glob('*') if p.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']]\n",
    "        random.shuffle(images)\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * split_ratio[0])\n",
    "        n_val = int(n_total * split_ratio[1])\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        splits = {\n",
    "            'train': images[:n_train],\n",
    "            'val': images[n_train:n_train+n_val],\n",
    "            'test': images[n_train+n_val:]\n",
    "        }\n",
    "\n",
    "        for split, img_list in splits.items():\n",
    "            for img_path in img_list:\n",
    "                dest = output_root / f\"orFold{fold}\" / split / cat / img_path.name\n",
    "                shutil.copy2(img_path, dest)\n",
    "\n",
    "    print(f\"Fold {fold} created in {fold_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold1\\train -> ..\\data\\png4ch\\224px\\orFold1\\train_aug\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\absence\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\absence\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\presence-defect\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\presence-defect\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold2\\train -> ..\\data\\png4ch\\224px\\orFold2\\train_aug\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold1\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold2\\train -> ..\\data\\png4ch\\224px\\orFold2\\train_aug\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\absence\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\absence\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\presence-defect\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\presence-defect\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold3\\train -> ..\\data\\png4ch\\224px\\orFold3\\train_aug\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold2\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold3\\train -> ..\\data\\png4ch\\224px\\orFold3\\train_aug\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\absence\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\absence\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\presence-defect\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\presence-defect\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold4\\train -> ..\\data\\png4ch\\224px\\orFold4\\train_aug\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold3\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold4\\train -> ..\\data\\png4ch\\224px\\orFold4\\train_aug\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\absence\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\absence\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\presence-defect\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\presence-defect\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold5\\train -> ..\\data\\png4ch\\224px\\orFold5\\train_aug\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold4\\train_aug\\presence-good\n",
      "Aumentando imágenes en: ..\\data\\png4ch\\224px\\orFold5\\train -> ..\\data\\png4ch\\224px\\orFold5\\train_aug\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\absence\n",
      "Categoría: absence | Originales válidas: 56 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\absence\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\presence-defect\n",
      "Categoría: presence-defect | Originales válidas: 144 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\presence-defect\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\presence-good\n",
      "Categoría: presence-good | Originales válidas: 35 | Total guardadas: 1000 en ..\\data\\png4ch\\224px\\orFold5\\train_aug\\presence-good\n"
     ]
    }
   ],
   "source": [
    "# Use augment_images to create train_aug in each fold\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base path for folds\n",
    "# base_folds = Path(\"../data/imgShiny/224px\")\n",
    "base_folds = Path(\"../data/png4ch/224px\")\n",
    "n_folds = 5\n",
    "target_count = 1000 \n",
    "\n",
    "for fold in range(1, n_folds+1):\n",
    "    fold_dir = base_folds / f\"orFold{fold}\"\n",
    "    train_dir = fold_dir / \"train\"\n",
    "    train_aug_dir = fold_dir / \"train_aug\"\n",
    "    print(f\"Augmenting images in: {train_dir} -> {train_aug_dir}\")\n",
    "    augment_images(str(train_dir), str(train_aug_dir), target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acc8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting torch==2.7.1+cu118 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nalex\\onedrive\\documentos\\phd\\vsprojects\\bga_defectclass\\venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Collecting filelock (from torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.7.1+cu118->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.1+cu118->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 536.2/536.2 kB 13.2 MB/s  0:00:00\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.7.1+cu118->torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5.tar.gz (19 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 34.6 MB/s  0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl (2817.2 MB)\n",
      "   ---------------------------------------- 0.0/2.8 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 GB 37.8 MB/s eta 0:01:15\n",
      "   ---------------------------------------- 0.0/2.8 GB 37.9 MB/s eta 0:01:14\n",
      "   ---------------------------------------- 0.0/2.8 GB 43.1 MB/s eta 0:01:05\n",
      "    --------------------------------------- 0.0/2.8 GB 50.0 MB/s eta 0:00:56\n",
      "    --------------------------------------- 0.1/2.8 GB 53.1 MB/s eta 0:00:53\n",
      "    --------------------------------------- 0.1/2.8 GB 53.5 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 0.1/2.8 GB 54.8 MB/s eta 0:00:50\n",
      "   - -------------------------------------- 0.1/2.8 GB 53.2 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 0.1/2.8 GB 51.8 MB/s eta 0:00:53\n",
      "   - -------------------------------------- 0.1/2.8 GB 50.4 MB/s eta 0:00:54\n",
      "   - -------------------------------------- 0.1/2.8 GB 49.5 MB/s eta 0:00:55\n",
      "   - -------------------------------------- 0.1/2.8 GB 48.7 MB/s eta 0:00:56\n",
      "   - -------------------------------------- 0.1/2.8 GB 49.3 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.1/2.8 GB 49.4 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 49.2 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 49.0 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 48.8 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 48.5 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 48.5 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 48.4 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 0.2/2.8 GB 45.6 MB/s eta 0:00:58\n",
      "   -- ------------------------------------- 0.2/2.8 GB 46.3 MB/s eta 0:00:57\n",
      "   --- ------------------------------------ 0.2/2.8 GB 47.4 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.2/2.8 GB 47.4 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.2/2.8 GB 47.4 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.3/2.8 GB 47.1 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.3/2.8 GB 46.9 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.3/2.8 GB 47.0 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 0.3/2.8 GB 47.2 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 47.3 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 46.8 MB/s eta 0:00:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 46.0 MB/s eta 0:00:55\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 45.6 MB/s eta 0:00:55\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 45.2 MB/s eta 0:00:56\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 44.7 MB/s eta 0:00:56\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 45.0 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 44.3 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 44.3 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 44.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 43.9 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 44.0 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 43.6 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 43.7 MB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 44.1 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 0.4/2.8 GB 44.8 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 0.4/2.8 GB 44.4 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 0.4/2.8 GB 44.5 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 44.3 MB/s eta 0:00:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 45.7 MB/s eta 0:00:52\n",
      "   ------ --------------------------------- 0.5/2.8 GB 45.2 MB/s eta 0:00:52\n",
      "   ------ --------------------------------- 0.5/2.8 GB 44.7 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 0.5/2.8 GB 44.3 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 0.5/2.8 GB 44.6 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 0.5/2.8 GB 44.9 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 0.5/2.8 GB 45.0 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 0.5/2.8 GB 45.3 MB/s eta 0:00:51\n",
      "   ------- -------------------------------- 0.5/2.8 GB 45.2 MB/s eta 0:00:51\n",
      "   ------- -------------------------------- 0.6/2.8 GB 44.8 MB/s eta 0:00:51\n",
      "   ------- -------------------------------- 0.6/2.8 GB 44.7 MB/s eta 0:00:51\n",
      "   -------- ------------------------------- 0.6/2.8 GB 44.6 MB/s eta 0:00:51\n",
      "   -------- ------------------------------- 0.6/2.8 GB 45.1 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 0.6/2.8 GB 45.0 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 0.6/2.8 GB 45.0 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 0.6/2.8 GB 44.8 MB/s eta 0:00:50\n",
      "   -------- ------------------------------- 0.6/2.8 GB 45.1 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 0.6/2.8 GB 45.6 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 0.6/2.8 GB 46.3 MB/s eta 0:00:48\n",
      "   --------- ------------------------------ 0.6/2.8 GB 46.4 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 0.7/2.8 GB 46.3 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 0.7/2.8 GB 46.2 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 0.7/2.8 GB 45.9 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 0.7/2.8 GB 45.5 MB/s eta 0:00:47\n",
      "   --------- ------------------------------ 0.7/2.8 GB 44.9 MB/s eta 0:00:48\n",
      "   --------- ------------------------------ 0.7/2.8 GB 45.0 MB/s eta 0:00:48\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 45.0 MB/s eta 0:00:47\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 45.2 MB/s eta 0:00:47\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 45.5 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 45.5 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 45.3 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 45.3 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 44.9 MB/s eta 0:00:46\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 44.9 MB/s eta 0:00:46\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 45.0 MB/s eta 0:00:46\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 45.0 MB/s eta 0:00:46\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 44.8 MB/s eta 0:00:46\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 44.8 MB/s eta 0:00:45\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 45.0 MB/s eta 0:00:45\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 44.8 MB/s eta 0:00:45\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 44.9 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 0.8/2.8 GB 44.8 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.7 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.7 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.6 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.8 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.7 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 44.3 MB/s eta 0:00:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 43.3 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 0.9/2.8 GB 43.2 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 0.9/2.8 GB 43.1 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 0.9/2.8 GB 43.3 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 0.9/2.8 GB 43.8 MB/s eta 0:00:43\n",
      "   ------------- -------------------------- 1.0/2.8 GB 44.4 MB/s eta 0:00:42\n",
      "   ------------- -------------------------- 1.0/2.8 GB 44.4 MB/s eta 0:00:42\n",
      "   ------------- -------------------------- 1.0/2.8 GB 44.6 MB/s eta 0:00:42\n",
      "   ------------- -------------------------- 1.0/2.8 GB 44.6 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.4 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.5 MB/s eta 0:00:41\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.5 MB/s eta 0:00:41\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.6 MB/s eta 0:00:41\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.6 MB/s eta 0:00:41\n",
      "   -------------- ------------------------- 1.0/2.8 GB 44.5 MB/s eta 0:00:40\n",
      "   -------------- ------------------------- 1.1/2.8 GB 44.6 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:40\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:39\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:39\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.6 MB/s eta 0:00:39\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:39\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.7 MB/s eta 0:00:39\n",
      "   --------------- ------------------------ 1.1/2.8 GB 44.8 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 44.7 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 44.9 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 44.7 MB/s eta 0:00:38\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 46.3 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 46.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 46.4 MB/s eta 0:00:36\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 46.4 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 45.9 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 45.5 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 45.3 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 44.8 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 44.9 MB/s eta 0:00:36\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 44.9 MB/s eta 0:00:35\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 45.0 MB/s eta 0:00:35\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 44.9 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.0 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.0 MB/s eta 0:00:35\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.1 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.2 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.1 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.1 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 1.3/2.8 GB 45.1 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 1.3/2.8 GB 45.2 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.2 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.2 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.1 MB/s eta 0:00:33\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.2 MB/s eta 0:00:32\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.0 MB/s eta 0:00:32\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.0 MB/s eta 0:00:32\n",
      "   ------------------- -------------------- 1.4/2.8 GB 45.0 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 1.4/2.8 GB 45.0 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 1.4/2.8 GB 45.0 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 1.4/2.8 GB 45.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 1.4/2.8 GB 45.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 1.5/2.8 GB 45.1 MB/s eta 0:00:31\n",
      "   -------------------- ------------------- 1.5/2.8 GB 45.1 MB/s eta 0:00:30\n",
      "   -------------------- ------------------- 1.5/2.8 GB 45.1 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 45.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 45.2 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 45.1 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 45.2 MB/s eta 0:00:29\n",
      "   --------------------- ------------------ 1.5/2.8 GB 45.3 MB/s eta 0:00:29\n",
      "   --------------------- ------------------ 1.5/2.8 GB 43.3 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 44.0 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 44.9 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.7 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.7 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.8 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.8 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.8 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.9 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 44.9 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 45.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 45.0 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 44.9 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 44.8 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 44.7 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.8 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 1.7/2.8 GB 44.9 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 1.8/2.8 GB 44.8 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 1.8/2.8 GB 44.8 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 1.8/2.8 GB 44.2 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 1.8/2.8 GB 43.8 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 1.8/2.8 GB 45.4 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 1.8/2.8 GB 45.0 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 1.8/2.8 GB 44.7 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 1.8/2.8 GB 45.1 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 1.8/2.8 GB 45.1 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 1.8/2.8 GB 44.8 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 1.8/2.8 GB 44.5 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 1.8/2.8 GB 44.0 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 44.1 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 1.9/2.8 GB 44.1 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 1.9/2.8 GB 41.4 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 1.9/2.8 GB 40.7 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 1.9/2.8 GB 39.7 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 38.0 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 36.9 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 36.9 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 35.6 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 1.9/2.8 GB 34.8 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 33.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 1.9/2.8 GB 29.5 MB/s eta 0:00:33\n",
      "   -------------------------- ------------- 1.9/2.8 GB 29.0 MB/s eta 0:00:33\n",
      "   -------------------------- ------------- 1.9/2.8 GB 28.2 MB/s eta 0:00:34\n",
      "   -------------------------- ------------- 1.9/2.8 GB 28.2 MB/s eta 0:00:34\n",
      "   -------------------------- ------------- 1.9/2.8 GB 27.0 MB/s eta 0:00:35\n",
      "   -------------------------- ------------- 1.9/2.8 GB 26.7 MB/s eta 0:00:36\n",
      "   -------------------------- ------------- 1.9/2.8 GB 26.1 MB/s eta 0:00:37\n",
      "   -------------------------- ------------- 1.9/2.8 GB 25.7 MB/s eta 0:00:37\n",
      "   -------------------------- ------------- 1.9/2.8 GB 25.2 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.9/2.8 GB 25.0 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.9/2.8 GB 24.3 MB/s eta 0:00:39\n",
      "   -------------------------- ------------- 1.9/2.8 GB 23.9 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.9/2.8 GB 23.5 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.9/2.8 GB 23.3 MB/s eta 0:00:41\n",
      "   -------------------------- ------------- 1.9/2.8 GB 22.8 MB/s eta 0:00:41\n",
      "   -------------------------- ------------- 1.9/2.8 GB 22.4 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 1.9/2.8 GB 22.1 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 1.9/2.8 GB 21.9 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 1.9/2.8 GB 21.4 MB/s eta 0:00:44\n",
      "   -------------------------- ------------- 1.9/2.8 GB 21.4 MB/s eta 0:00:44\n",
      "   -------------------------- ------------- 1.9/2.8 GB 21.4 MB/s eta 0:00:44\n",
      "   -------------------------- ------------- 1.9/2.8 GB 20.7 MB/s eta 0:00:45\n",
      "   -------------------------- ------------- 1.9/2.8 GB 20.3 MB/s eta 0:00:46\n",
      "   -------------------------- ------------- 1.9/2.8 GB 20.0 MB/s eta 0:00:47\n",
      "   -------------------------- ------------- 1.9/2.8 GB 19.9 MB/s eta 0:00:47\n",
      "   -------------------------- ------------- 1.9/2.8 GB 19.5 MB/s eta 0:00:48\n",
      "   -------------------------- ------------- 1.9/2.8 GB 19.3 MB/s eta 0:00:48\n",
      "   -------------------------- ------------- 1.9/2.8 GB 18.9 MB/s eta 0:00:49\n",
      "   --------------------------- ------------ 1.9/2.8 GB 18.7 MB/s eta 0:00:49\n",
      "   --------------------------- ------------ 1.9/2.8 GB 18.5 MB/s eta 0:00:50\n",
      "   --------------------------- ------------ 1.9/2.8 GB 18.3 MB/s eta 0:00:50\n",
      "   --------------------------- ------------ 1.9/2.8 GB 18.1 MB/s eta 0:00:51\n",
      "   --------------------------- ------------ 1.9/2.8 GB 17.9 MB/s eta 0:00:51\n",
      "   --------------------------- ------------ 1.9/2.8 GB 17.7 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 1.9/2.8 GB 17.6 MB/s eta 0:00:52\n",
      "   --------------------------- ------------ 1.9/2.8 GB 17.3 MB/s eta 0:00:53\n",
      "   --------------------------- ------------ 1.9/2.8 GB 17.2 MB/s eta 0:00:53\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.9 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.8 MB/s eta 0:00:54\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.5 MB/s eta 0:00:55\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.4 MB/s eta 0:00:55\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.2 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 1.9/2.8 GB 16.1 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.9 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.8 MB/s eta 0:00:56\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.6 MB/s eta 0:00:57\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.5 MB/s eta 0:00:57\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.3 MB/s eta 0:00:58\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.2 MB/s eta 0:00:58\n",
      "   --------------------------- ------------ 1.9/2.8 GB 15.0 MB/s eta 0:00:59\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.9 MB/s eta 0:00:59\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.7 MB/s eta 0:01:00\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.6 MB/s eta 0:01:00\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.4 MB/s eta 0:01:01\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.3 MB/s eta 0:01:01\n",
      "   --------------------------- ------------ 1.9/2.8 GB 14.2 MB/s eta 0:01:02\n",
      "   --------------------------- ------------ 2.0/2.8 GB 14.0 MB/s eta 0:01:02\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.9 MB/s eta 0:01:03\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.8 MB/s eta 0:01:03\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.7 MB/s eta 0:01:03\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.6 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.4 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.3 MB/s eta 0:01:05\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.2 MB/s eta 0:01:05\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.1 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 2.0/2.8 GB 13.0 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 2.0/2.8 GB 12.9 MB/s eta 0:01:07\n",
      "   --------------------------- ------------ 2.0/2.8 GB 12.7 MB/s eta 0:01:07\n",
      "   --------------------------- ------------ 2.0/2.8 GB 12.6 MB/s eta 0:01:08\n",
      "   --------------------------- ------------ 2.0/2.8 GB 12.5 MB/s eta 0:01:08\n",
      "   --------------------------- ------------ 2.0/2.8 GB 12.4 MB/s eta 0:01:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 12.3 MB/s eta 0:01:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 12.3 MB/s eta 0:01:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 12.1 MB/s eta 0:01:10\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 12.1 MB/s eta 0:01:10\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 12.0 MB/s eta 0:01:10\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.9 MB/s eta 0:01:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.8 MB/s eta 0:01:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.8 MB/s eta 0:01:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.6 MB/s eta 0:01:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.6 MB/s eta 0:01:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.5 MB/s eta 0:01:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.3 MB/s eta 0:01:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.3 MB/s eta 0:01:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.2 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.2 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 11.1 MB/s eta 0:01:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.4 MB/s eta 0:04:00\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:53\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.5 MB/s eta 0:03:54\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.1 MB/s eta 0:04:23\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.1 MB/s eta 0:04:23\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.0 MB/s eta 0:04:27\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.0 MB/s eta 0:04:28\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.0 MB/s eta 0:04:32\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.0 MB/s eta 0:04:32\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:42\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:42\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:43\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:43\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.7 MB/s eta 0:04:51\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:46\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.7 MB/s eta 0:04:51\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:42\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:39\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.8 MB/s eta 0:04:46\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 2.9 MB/s eta 0:04:34\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.0 MB/s eta 0:04:20\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 3.1 MB/s eta 0:04:10\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 3.1 MB/s eta 0:04:11\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 3.1 MB/s eta 0:04:11\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 3.1 MB/s eta 0:04:10\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:04:02\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:03:56\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:03:58\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.3 MB/s eta 0:03:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.4 MB/s eta 0:03:45\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.4 MB/s eta 0:03:42\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.4 MB/s eta 0:03:42\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.4 MB/s eta 0:03:42\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.4 MB/s eta 0:03:42\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:03:54\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.2 MB/s eta 0:03:54\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.1 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.1 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.1 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.1 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.1 MB/s eta 0:03:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:05\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:05\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:05\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:04\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:06\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 3.0 MB/s eta 0:04:10\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 2.9 MB/s eta 0:04:11\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 2.9 MB/s eta 0:04:15\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 2.9 MB/s eta 0:04:18\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 2.9 MB/s eta 0:04:18\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.0 MB/s eta 0:01:46\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.0 MB/s eta 0:01:46\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.9 MB/s eta 0:01:46\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:41\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:41\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:40\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:40\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:40\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.2 MB/s eta 0:01:40\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.0 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.0 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 7.0 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.9 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.9 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.9 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.9 MB/s eta 0:01:44\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.6 MB/s eta 0:01:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.2 MB/s eta 0:01:57\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.1 MB/s eta 0:01:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.1 MB/s eta 0:01:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.1 MB/s eta 0:01:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.1 MB/s eta 0:01:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 6.1 MB/s eta 0:01:59\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.8 MB/s eta 0:02:04\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.8 MB/s eta 0:02:04\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.8 MB/s eta 0:02:04\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.8 MB/s eta 0:02:04\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.6 MB/s eta 0:02:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.6 MB/s eta 0:02:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.6 MB/s eta 0:02:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.6 MB/s eta 0:02:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.5 MB/s eta 0:02:09\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.5 MB/s eta 0:02:09\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.5 MB/s eta 0:02:09\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.4 MB/s eta 0:02:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.4 MB/s eta 0:02:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.4 MB/s eta 0:02:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.4 MB/s eta 0:02:13\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.3 MB/s eta 0:02:14\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.3 MB/s eta 0:02:15\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.3 MB/s eta 0:02:15\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.3 MB/s eta 0:02:15\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 5.3 MB/s eta 0:02:15\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.3 MB/s eta 0:02:13\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.3 MB/s eta 0:02:13\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.3 MB/s eta 0:02:14\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.2 MB/s eta 0:02:14\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.3 MB/s eta 0:02:12\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.5 MB/s eta 0:02:05\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.6 MB/s eta 0:02:04\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.7 MB/s eta 0:02:01\n",
      "   ------------------------------ --------- 2.1/2.8 GB 5.9 MB/s eta 0:01:55\n",
      "   ------------------------------ --------- 2.1/2.8 GB 6.1 MB/s eta 0:01:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 6.1 MB/s eta 0:01:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 6.2 MB/s eta 0:01:48\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.4 MB/s eta 0:01:44\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.6 MB/s eta 0:01:40\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.6 MB/s eta 0:01:40\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.7 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.8 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.8 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.7 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.7 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.7 MB/s eta 0:01:37\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.7 MB/s eta 0:01:36\n",
      "   ------------------------------ --------- 2.2/2.8 GB 6.9 MB/s eta 0:01:33\n",
      "   ------------------------------- -------- 2.2/2.8 GB 7.0 MB/s eta 0:01:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 7.4 MB/s eta 0:01:24\n",
      "   ------------------------------- -------- 2.2/2.8 GB 7.6 MB/s eta 0:01:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.2 MB/s eta 0:01:12\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.7 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.5 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 8.3 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.3 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.2 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.2 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.3/2.8 GB 8.1 MB/s eta 0:01:10\n",
      "   -------------------------------- ------- 2.3/2.8 GB 6.6 MB/s eta 0:01:25\n",
      "   -------------------------------- ------- 2.3/2.8 GB 6.7 MB/s eta 0:01:24\n",
      "   -------------------------------- ------- 2.3/2.8 GB 6.8 MB/s eta 0:01:22\n",
      "   -------------------------------- ------- 2.3/2.8 GB 6.9 MB/s eta 0:01:20\n",
      "   -------------------------------- ------- 2.3/2.8 GB 7.5 MB/s eta 0:01:11\n",
      "   -------------------------------- ------- 2.3/2.8 GB 8.0 MB/s eta 0:01:05\n",
      "   -------------------------------- ------- 2.3/2.8 GB 8.4 MB/s eta 0:01:00\n",
      "   --------------------------------- ------ 2.3/2.8 GB 8.7 MB/s eta 0:00:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 9.3 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 2.4/2.8 GB 9.7 MB/s eta 0:00:49\n",
      "   --------------------------------- ------ 2.4/2.8 GB 10.5 MB/s eta 0:00:44\n",
      "   --------------------------------- ------ 2.4/2.8 GB 13.5 MB/s eta 0:00:34\n",
      "   --------------------------------- ------ 2.4/2.8 GB 14.6 MB/s eta 0:00:30\n",
      "   --------------------------------- ------ 2.4/2.8 GB 14.9 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 15.0 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 8.3 MB/s eta 0:00:51\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 7.1 MB/s eta 0:00:59\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 6.7 MB/s eta 0:01:03\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 6.7 MB/s eta 0:01:03\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 6.7 MB/s eta 0:01:03\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 6.7 MB/s eta 0:01:03\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.4 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.3 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.3 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.3 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.3 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.4 MB/s eta 0:01:17\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.4 MB/s eta 0:01:17\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:14\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:13\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:13\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:14\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:13\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.6 MB/s eta 0:01:12\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.7 MB/s eta 0:01:11\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.7 MB/s eta 0:01:11\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.8 MB/s eta 0:01:09\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.8 MB/s eta 0:01:08\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.8 MB/s eta 0:01:08\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.8 MB/s eta 0:01:08\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 5.8 MB/s eta 0:01:09\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 7.3 MB/s eta 0:00:54\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 7.5 MB/s eta 0:00:52\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 8.1 MB/s eta 0:00:45\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 8.4 MB/s eta 0:00:43\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 8.5 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 8.4 MB/s eta 0:00:43\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:42\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.4 MB/s eta 0:00:41\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.5 MB/s eta 0:00:40\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.6 MB/s eta 0:00:39\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:39\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:38\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.9 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.9 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.9 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.9 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.8 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.8 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.8 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 8.7 MB/s eta 0:00:36\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.3 MB/s eta 0:00:59\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.2 MB/s eta 0:01:00\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.2 MB/s eta 0:01:00\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.2 MB/s eta 0:01:00\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.2 MB/s eta 0:01:00\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 5.2 MB/s eta 0:01:00\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.8 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:47\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:48\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:48\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.5 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.5 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.5 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.5 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:47\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:46\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:44\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:44\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:44\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:44\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.6 MB/s eta 0:00:44\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:45\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:45\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 6.4 MB/s eta 0:00:45\n",
      "   ------------------------------------ --- 2.5/2.8 GB 6.4 MB/s eta 0:00:44\n",
      "   ------------------------------------ --- 2.5/2.8 GB 6.4 MB/s eta 0:00:44\n",
      "   ------------------------------------ --- 2.5/2.8 GB 6.6 MB/s eta 0:00:42\n",
      "   ------------------------------------ --- 2.5/2.8 GB 6.7 MB/s eta 0:00:41\n",
      "   ------------------------------------ --- 2.6/2.8 GB 6.9 MB/s eta 0:00:39\n",
      "   ------------------------------------ --- 2.6/2.8 GB 6.9 MB/s eta 0:00:38\n",
      "   ------------------------------------ --- 2.6/2.8 GB 6.9 MB/s eta 0:00:38\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:38\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:36\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:36\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 6.8 MB/s eta 0:00:37\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:35\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:35\n",
      "   ------------------------------------ --- 2.6/2.8 GB 6.9 MB/s eta 0:00:36\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:34\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:34\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.0 MB/s eta 0:00:34\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:33\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.1 MB/s eta 0:00:31\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.3 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------ --- 2.6/2.8 GB 7.2 MB/s eta 0:00:30\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.2 MB/s eta 0:00:29\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.3 MB/s eta 0:00:28\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.5 MB/s eta 0:00:26\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.7 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.8 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.9 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.9 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.9 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 8.0 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.6/2.8 GB 7.9 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.9 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.9 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.9 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.9 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.9 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.7 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.7 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.7 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.6 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.6 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.6 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.5 MB/s eta 0:00:24\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.6 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 2.7/2.8 GB 6.8 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.8 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.8 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.8 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.6 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.6 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.6 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.6 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.5 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.5 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.5 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.5 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.5 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 6.3 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.0 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.0 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.0 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.0 MB/s eta 0:00:17\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.2 MB/s eta 0:00:16\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:15\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.3 MB/s eta 0:00:15\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.3 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.3 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.3 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.5 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.5 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.4 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.6 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.6 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.6 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.6 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.7 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.7 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 2.7/2.8 GB 7.8 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 7.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 7.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 7.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 7.5 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 7.5 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.8/2.8 GB 7.5 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.8/2.8 GB 7.8 MB/s eta 0:00:08\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 8.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.8 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 7.5 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.6 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 GB ?  0:05:41\n",
      "Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 41.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 41.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 41.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.4 MB/s  0:00:00\n",
      "Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 32.1 MB/s  0:00:00\n",
      "Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "Building wheels for collected packages: MarkupSafe\n",
      "  Building wheel for MarkupSafe (pyproject.toml): started\n",
      "  Building wheel for MarkupSafe (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for MarkupSafe: filename=MarkupSafe-2.1.5-py3-none-any.whl size=9913 sha256=ead97ae229f9e7221b498cdb4e139c8b927c159a88bf096fc3bb03b06c324fd0\n",
      "  Stored in directory: c:\\users\\nalex\\appdata\\local\\pip\\cache\\wheels\\84\\b6\\28\\95b8e298901ee19b488797095efb5fbed28637ed83215b13b1\n",
      "Successfully built MarkupSafe\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [sympy]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ---------- -----------------------------  3/12 [setuptools]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [numpy]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [networkx]\n",
      "   -------------------- -------------------  6/12 [MarkupSafe]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [fsspec]\n",
      "   -------------------------- -------------  8/12 [filelock]\n",
      "   -------------------------- -------------  8/12 [filelock]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [jinja2]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torch]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchvision]\n",
      "   ---------------------------------------- 12/12 [torchvision]\n",
      "\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 setuptools-70.2.0 sympy-1.14.0 torch-2.7.1+cu118 torchvision-0.22.1+cu118 typing-extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806802d3",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=16 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 114.3150 | val_loss: 0.6813 | train_acc: 0.7353 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 01 | train_loss: 114.3150 | val_loss: 0.6813 | train_acc: 0.7353 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 02 | train_loss: 74.9507 | val_loss: 3.1267 | train_acc: 0.8407 | val_acc: 0.8571 | val_F1: 0.8439\n",
      "Epoch 02 | train_loss: 74.9507 | val_loss: 3.1267 | train_acc: 0.8407 | val_acc: 0.8571 | val_F1: 0.8439\n",
      "Epoch 03 | train_loss: 63.2328 | val_loss: 0.2494 | train_acc: 0.8627 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 03 | train_loss: 63.2328 | val_loss: 0.2494 | train_acc: 0.8627 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 04 | train_loss: 54.4170 | val_loss: 1.1131 | train_acc: 0.8853 | val_acc: 0.8163 | val_F1: 0.8240\n",
      "Epoch 04 | train_loss: 54.4170 | val_loss: 1.1131 | train_acc: 0.8853 | val_acc: 0.8163 | val_F1: 0.8240\n",
      "Epoch 05 | train_loss: 60.1600 | val_loss: 0.1984 | train_acc: 0.8700 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 05 | train_loss: 60.1600 | val_loss: 0.1984 | train_acc: 0.8700 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 47.5042 | val_loss: 1.7398 | train_acc: 0.8930 | val_acc: 0.7551 | val_F1: 0.7604\n",
      "Epoch 06 | train_loss: 47.5042 | val_loss: 1.7398 | train_acc: 0.8930 | val_acc: 0.7551 | val_F1: 0.7604\n",
      "Epoch 07 | train_loss: 45.9649 | val_loss: 0.6559 | train_acc: 0.9017 | val_acc: 0.9388 | val_F1: 0.9371\n",
      "Epoch 07 | train_loss: 45.9649 | val_loss: 0.6559 | train_acc: 0.9017 | val_acc: 0.9388 | val_F1: 0.9371\n",
      "Epoch 08 | train_loss: 45.6866 | val_loss: 1.3670 | train_acc: 0.9033 | val_acc: 0.8367 | val_F1: 0.8052\n",
      "Epoch 08 | train_loss: 45.6866 | val_loss: 1.3670 | train_acc: 0.9033 | val_acc: 0.8367 | val_F1: 0.8052\n",
      "Epoch 09 | train_loss: 38.2981 | val_loss: 0.2218 | train_acc: 0.9173 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 09 | train_loss: 38.2981 | val_loss: 0.2218 | train_acc: 0.9173 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 10 | train_loss: 39.4061 | val_loss: 0.8351 | train_acc: 0.9140 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 10 | train_loss: 39.4061 | val_loss: 0.8351 | train_acc: 0.9140 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 11 | train_loss: 36.4959 | val_loss: 1.4034 | train_acc: 0.9243 | val_acc: 0.8367 | val_F1: 0.8390\n",
      "Epoch 11 | train_loss: 36.4959 | val_loss: 1.4034 | train_acc: 0.9243 | val_acc: 0.8367 | val_F1: 0.8390\n",
      "Epoch 12 | train_loss: 38.1833 | val_loss: 0.2252 | train_acc: 0.9143 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 12 | train_loss: 38.1833 | val_loss: 0.2252 | train_acc: 0.9143 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 13 | train_loss: 36.3073 | val_loss: 0.0934 | train_acc: 0.9153 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 13 | train_loss: 36.3073 | val_loss: 0.0934 | train_acc: 0.9153 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 14 | train_loss: 31.9369 | val_loss: 0.6207 | train_acc: 0.9320 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 31.9369 | val_loss: 0.6207 | train_acc: 0.9320 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 33.0607 | val_loss: 0.0775 | train_acc: 0.9327 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "Epoch 15 | train_loss: 33.0607 | val_loss: 0.0775 | train_acc: 0.9327 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_135451_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_135451_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 61.2252 | val_loss: 0.2127 | train_acc: 0.8937 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 01 | train_loss: 61.2252 | val_loss: 0.2127 | train_acc: 0.8937 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 02 | train_loss: 30.8499 | val_loss: 0.2033 | train_acc: 0.9357 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 02 | train_loss: 30.8499 | val_loss: 0.2033 | train_acc: 0.9357 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 03 | train_loss: 27.7932 | val_loss: 0.1031 | train_acc: 0.9417 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 03 | train_loss: 27.7932 | val_loss: 0.1031 | train_acc: 0.9417 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 27.2344 | val_loss: 0.1212 | train_acc: 0.9427 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 27.2344 | val_loss: 0.1212 | train_acc: 0.9427 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 21.1131 | val_loss: 0.0983 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 21.1131 | val_loss: 0.0983 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 24.2789 | val_loss: 0.1351 | train_acc: 0.9487 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 24.2789 | val_loss: 0.1351 | train_acc: 0.9487 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 20.2451 | val_loss: 0.2709 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 20.2451 | val_loss: 0.2709 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 18.0155 | val_loss: 0.2474 | train_acc: 0.9603 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 18.0155 | val_loss: 0.2474 | train_acc: 0.9603 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 21.2327 | val_loss: 0.1122 | train_acc: 0.9527 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 21.2327 | val_loss: 0.1122 | train_acc: 0.9527 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 10 | train_loss: 19.9021 | val_loss: 0.0869 | train_acc: 0.9617 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 10 | train_loss: 19.9021 | val_loss: 0.0869 | train_acc: 0.9617 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 11 | train_loss: 17.6389 | val_loss: 0.1038 | train_acc: 0.9623 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "Epoch 11 | train_loss: 17.6389 | val_loss: 0.1038 | train_acc: 0.9623 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "-> Test F1: 1.0000 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_140335_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "-> Test F1: 1.0000 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_140335_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 25.0129 | val_loss: 0.2136 | train_acc: 0.8947 | val_acc: 0.9592 | val_F1: 0.9611\n",
      "Epoch 01 | train_loss: 25.0129 | val_loss: 0.2136 | train_acc: 0.8947 | val_acc: 0.9592 | val_F1: 0.9611\n",
      "Epoch 02 | train_loss: 21.0638 | val_loss: 0.4352 | train_acc: 0.9140 | val_acc: 0.8776 | val_F1: 0.8786\n",
      "Epoch 02 | train_loss: 21.0638 | val_loss: 0.4352 | train_acc: 0.9140 | val_acc: 0.8776 | val_F1: 0.8786\n",
      "Epoch 03 | train_loss: 16.0592 | val_loss: 0.2798 | train_acc: 0.9343 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 03 | train_loss: 16.0592 | val_loss: 0.2798 | train_acc: 0.9343 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 04 | train_loss: 19.4061 | val_loss: 0.0647 | train_acc: 0.9220 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 19.4061 | val_loss: 0.0647 | train_acc: 0.9220 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 16.9391 | val_loss: 0.0376 | train_acc: 0.9233 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 05 | train_loss: 16.9391 | val_loss: 0.0376 | train_acc: 0.9233 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 16.4461 | val_loss: 0.0650 | train_acc: 0.9280 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 16.4461 | val_loss: 0.0650 | train_acc: 0.9280 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 07 | train_loss: 15.4433 | val_loss: 0.2700 | train_acc: 0.9360 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 07 | train_loss: 15.4433 | val_loss: 0.2700 | train_acc: 0.9360 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 08 | train_loss: 14.2383 | val_loss: 0.0491 | train_acc: 0.9380 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 14.2383 | val_loss: 0.0491 | train_acc: 0.9380 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 14.8440 | val_loss: 0.8842 | train_acc: 0.9387 | val_acc: 0.8776 | val_F1: 0.8627\n",
      "Epoch 09 | train_loss: 14.8440 | val_loss: 0.8842 | train_acc: 0.9387 | val_acc: 0.8776 | val_F1: 0.8627\n",
      "Epoch 10 | train_loss: 13.5921 | val_loss: 0.1164 | train_acc: 0.9400 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 10 | train_loss: 13.5921 | val_loss: 0.1164 | train_acc: 0.9400 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 11 | train_loss: 13.2812 | val_loss: 0.4307 | train_acc: 0.9483 | val_acc: 0.8571 | val_F1: 0.8591\n",
      "Epoch 11 | train_loss: 13.2812 | val_loss: 0.4307 | train_acc: 0.9483 | val_acc: 0.8571 | val_F1: 0.8591\n",
      "Epoch 12 | train_loss: 14.0795 | val_loss: 0.0085 | train_acc: 0.9360 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 12 | train_loss: 14.0795 | val_loss: 0.0085 | train_acc: 0.9360 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 13 | train_loss: 12.6201 | val_loss: 0.1466 | train_acc: 0.9457 | val_acc: 0.9592 | val_F1: 0.9611\n",
      "Epoch 13 | train_loss: 12.6201 | val_loss: 0.1466 | train_acc: 0.9457 | val_acc: 0.9592 | val_F1: 0.9611\n",
      "Epoch 14 | train_loss: 14.1896 | val_loss: 0.4717 | train_acc: 0.9357 | val_acc: 0.8980 | val_F1: 0.9007\n",
      "Epoch 14 | train_loss: 14.1896 | val_loss: 0.4717 | train_acc: 0.9357 | val_acc: 0.8980 | val_F1: 0.9007\n",
      "Epoch 15 | train_loss: 12.0533 | val_loss: 0.1187 | train_acc: 0.9467 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "Epoch 15 | train_loss: 12.0533 | val_loss: 0.1187 | train_acc: 0.9467 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_150050_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_150050_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando resnet50 (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 38.7261 | val_loss: 0.0572 | train_acc: 0.8610 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 01 | train_loss: 38.7261 | val_loss: 0.0572 | train_acc: 0.8610 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 02 | train_loss: 12.5687 | val_loss: 0.0548 | train_acc: 0.9490 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 02 | train_loss: 12.5687 | val_loss: 0.0548 | train_acc: 0.9490 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 03 | train_loss: 10.2978 | val_loss: 0.0836 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 03 | train_loss: 10.2978 | val_loss: 0.0836 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 10.4692 | val_loss: 0.0705 | train_acc: 0.9603 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 10.4692 | val_loss: 0.0705 | train_acc: 0.9603 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 10.1020 | val_loss: 0.1295 | train_acc: 0.9583 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 10.1020 | val_loss: 0.1295 | train_acc: 0.9583 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 8.5615 | val_loss: 0.0968 | train_acc: 0.9640 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 8.5615 | val_loss: 0.0968 | train_acc: 0.9640 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 9.4875 | val_loss: 0.0887 | train_acc: 0.9597 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 9.4875 | val_loss: 0.0887 | train_acc: 0.9597 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 8.2259 | val_loss: 0.0941 | train_acc: 0.9653 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 8.2259 | val_loss: 0.0941 | train_acc: 0.9653 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 8.1099 | val_loss: 0.0383 | train_acc: 0.9653 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 09 | train_loss: 8.1099 | val_loss: 0.0383 | train_acc: 0.9653 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 7.0365 | val_loss: 0.0383 | train_acc: 0.9717 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 7.0365 | val_loss: 0.0383 | train_acc: 0.9717 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 11 | train_loss: 6.7536 | val_loss: 0.0630 | train_acc: 0.9713 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "Epoch 11 | train_loss: 6.7536 | val_loss: 0.0630 | train_acc: 0.9713 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_151406_img224_bs32_lr0.0001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=16 | lr=0.001 ===\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/resnet50_pretrained_20251209_151406_img224_bs32_lr0.0001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=16 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 99.6978 | val_loss: 0.8750 | train_acc: 0.7717 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 01 | train_loss: 99.6978 | val_loss: 0.8750 | train_acc: 0.7717 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 02 | train_loss: 66.0618 | val_loss: 0.6299 | train_acc: 0.8580 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 02 | train_loss: 66.0618 | val_loss: 0.6299 | train_acc: 0.8580 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 03 | train_loss: 56.5014 | val_loss: 0.6773 | train_acc: 0.8820 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 03 | train_loss: 56.5014 | val_loss: 0.6773 | train_acc: 0.8820 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 04 | train_loss: 47.7831 | val_loss: 0.1605 | train_acc: 0.9017 | val_acc: 0.9796 | val_F1: 0.9798\n",
      "Epoch 04 | train_loss: 47.7831 | val_loss: 0.1605 | train_acc: 0.9017 | val_acc: 0.9796 | val_F1: 0.9798\n",
      "Epoch 05 | train_loss: 43.5140 | val_loss: 0.1574 | train_acc: 0.9040 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 43.5140 | val_loss: 0.1574 | train_acc: 0.9040 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 39.2315 | val_loss: 0.1568 | train_acc: 0.9160 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 39.2315 | val_loss: 0.1568 | train_acc: 0.9160 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 07 | train_loss: 37.8017 | val_loss: 0.4412 | train_acc: 0.9217 | val_acc: 0.9184 | val_F1: 0.9195\n",
      "Epoch 07 | train_loss: 37.8017 | val_loss: 0.4412 | train_acc: 0.9217 | val_acc: 0.9184 | val_F1: 0.9195\n",
      "Epoch 08 | train_loss: 36.1156 | val_loss: 0.1870 | train_acc: 0.9277 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 36.1156 | val_loss: 0.1870 | train_acc: 0.9277 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 36.3639 | val_loss: 0.4169 | train_acc: 0.9267 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 09 | train_loss: 36.3639 | val_loss: 0.4169 | train_acc: 0.9267 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 10 | train_loss: 33.7468 | val_loss: 0.1784 | train_acc: 0.9290 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 10 | train_loss: 33.7468 | val_loss: 0.1784 | train_acc: 0.9290 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 11 | train_loss: 39.0228 | val_loss: 0.3093 | train_acc: 0.9177 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 11 | train_loss: 39.0228 | val_loss: 0.3093 | train_acc: 0.9177 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 12 | train_loss: 33.3166 | val_loss: 0.0922 | train_acc: 0.9337 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 12 | train_loss: 33.3166 | val_loss: 0.0922 | train_acc: 0.9337 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 13 | train_loss: 31.1709 | val_loss: 0.1414 | train_acc: 0.9330 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 31.1709 | val_loss: 0.1414 | train_acc: 0.9330 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 32.0515 | val_loss: 1.7467 | train_acc: 0.9363 | val_acc: 0.8980 | val_F1: 0.8917\n",
      "Epoch 14 | train_loss: 32.0515 | val_loss: 1.7467 | train_acc: 0.9363 | val_acc: 0.8980 | val_F1: 0.8917\n",
      "Epoch 15 | train_loss: 28.4848 | val_loss: 0.1473 | train_acc: 0.9383 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 15 | train_loss: 28.4848 | val_loss: 0.1473 | train_acc: 0.9383 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 16 | train_loss: 27.3672 | val_loss: 0.3171 | train_acc: 0.9437 | val_acc: 0.9592 | val_F1: 0.9586\n",
      "Early stopping activado.\n",
      "Epoch 16 | train_loss: 27.3672 | val_loss: 0.3171 | train_acc: 0.9437 | val_acc: 0.9592 | val_F1: 0.9586\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9810 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_152112_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "-> Test F1: 0.9810 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_152112_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 62.0201 | val_loss: 0.2535 | train_acc: 0.9023 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 01 | train_loss: 62.0201 | val_loss: 0.2535 | train_acc: 0.9023 | val_acc: 0.9796 | val_F1: 0.9801\n",
      "Epoch 02 | train_loss: 28.5810 | val_loss: 0.1543 | train_acc: 0.9493 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 02 | train_loss: 28.5810 | val_loss: 0.1543 | train_acc: 0.9493 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 03 | train_loss: 22.1688 | val_loss: 0.0895 | train_acc: 0.9583 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 03 | train_loss: 22.1688 | val_loss: 0.0895 | train_acc: 0.9583 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 04 | train_loss: 20.7844 | val_loss: 0.1413 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 20.7844 | val_loss: 0.1413 | train_acc: 0.9567 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 18.2949 | val_loss: 0.1292 | train_acc: 0.9677 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 18.2949 | val_loss: 0.1292 | train_acc: 0.9677 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 18.5833 | val_loss: 0.1628 | train_acc: 0.9613 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 18.5833 | val_loss: 0.1628 | train_acc: 0.9613 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 17.4462 | val_loss: 0.0962 | train_acc: 0.9670 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 17.4462 | val_loss: 0.0962 | train_acc: 0.9670 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 08 | train_loss: 17.7639 | val_loss: 0.0928 | train_acc: 0.9683 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 08 | train_loss: 17.7639 | val_loss: 0.0928 | train_acc: 0.9683 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 09 | train_loss: 16.9172 | val_loss: 0.0813 | train_acc: 0.9643 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 16.9172 | val_loss: 0.0813 | train_acc: 0.9643 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 10 | train_loss: 16.8303 | val_loss: 0.0477 | train_acc: 0.9643 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 16.8303 | val_loss: 0.0477 | train_acc: 0.9643 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 11 | train_loss: 14.1155 | val_loss: 0.0257 | train_acc: 0.9713 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 11 | train_loss: 14.1155 | val_loss: 0.0257 | train_acc: 0.9713 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 12 | train_loss: 14.3507 | val_loss: 0.0197 | train_acc: 0.9730 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "Epoch 12 | train_loss: 14.3507 | val_loss: 0.0197 | train_acc: 0.9730 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9811 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_152630_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "-> Test F1: 0.9811 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_152630_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 18.9255 | val_loss: 0.0863 | train_acc: 0.9263 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 01 | train_loss: 18.9255 | val_loss: 0.0863 | train_acc: 0.9263 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 02 | train_loss: 13.9262 | val_loss: 0.2333 | train_acc: 0.9447 | val_acc: 0.9184 | val_F1: 0.9128\n",
      "Epoch 02 | train_loss: 13.9262 | val_loss: 0.2333 | train_acc: 0.9447 | val_acc: 0.9184 | val_F1: 0.9128\n",
      "Epoch 03 | train_loss: 14.5420 | val_loss: 0.0178 | train_acc: 0.9347 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 03 | train_loss: 14.5420 | val_loss: 0.0178 | train_acc: 0.9347 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 04 | train_loss: 14.0039 | val_loss: 0.0478 | train_acc: 0.9463 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 04 | train_loss: 14.0039 | val_loss: 0.0478 | train_acc: 0.9463 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 14.0643 | val_loss: 0.0460 | train_acc: 0.9453 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 05 | train_loss: 14.0643 | val_loss: 0.0460 | train_acc: 0.9453 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 10.7206 | val_loss: 0.0129 | train_acc: 0.9597 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 06 | train_loss: 10.7206 | val_loss: 0.0129 | train_acc: 0.9597 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 07 | train_loss: 13.8420 | val_loss: 0.0980 | train_acc: 0.9457 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 07 | train_loss: 13.8420 | val_loss: 0.0980 | train_acc: 0.9457 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 08 | train_loss: 10.9954 | val_loss: 0.6766 | train_acc: 0.9567 | val_acc: 0.8776 | val_F1: 0.8704\n",
      "Epoch 08 | train_loss: 10.9954 | val_loss: 0.6766 | train_acc: 0.9567 | val_acc: 0.8776 | val_F1: 0.8704\n",
      "Epoch 09 | train_loss: 10.7300 | val_loss: 0.3391 | train_acc: 0.9547 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 09 | train_loss: 10.7300 | val_loss: 0.3391 | train_acc: 0.9547 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 10 | train_loss: 9.9624 | val_loss: 0.0269 | train_acc: 0.9587 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 9.9624 | val_loss: 0.0269 | train_acc: 0.9587 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 11 | train_loss: 10.6933 | val_loss: 0.0550 | train_acc: 0.9530 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "Epoch 11 | train_loss: 10.6933 | val_loss: 0.0550 | train_acc: 0.9530 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9624 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_153113_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "-> Test F1: 0.9624 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_153113_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando mobilenet (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 35.3946 | val_loss: 0.1394 | train_acc: 0.8923 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 01 | train_loss: 35.3946 | val_loss: 0.1394 | train_acc: 0.8923 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 02 | train_loss: 12.4963 | val_loss: 0.0645 | train_acc: 0.9610 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 02 | train_loss: 12.4963 | val_loss: 0.0645 | train_acc: 0.9610 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 03 | train_loss: 9.2901 | val_loss: 0.0375 | train_acc: 0.9653 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 03 | train_loss: 9.2901 | val_loss: 0.0375 | train_acc: 0.9653 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 04 | train_loss: 8.5864 | val_loss: 0.0319 | train_acc: 0.9660 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 04 | train_loss: 8.5864 | val_loss: 0.0319 | train_acc: 0.9660 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 05 | train_loss: 8.6636 | val_loss: 0.0647 | train_acc: 0.9677 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 05 | train_loss: 8.6636 | val_loss: 0.0647 | train_acc: 0.9677 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 7.8691 | val_loss: 0.0894 | train_acc: 0.9693 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 06 | train_loss: 7.8691 | val_loss: 0.0894 | train_acc: 0.9693 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 07 | train_loss: 7.2234 | val_loss: 0.0235 | train_acc: 0.9727 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 07 | train_loss: 7.2234 | val_loss: 0.0235 | train_acc: 0.9727 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 08 | train_loss: 7.5737 | val_loss: 0.0213 | train_acc: 0.9707 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 08 | train_loss: 7.5737 | val_loss: 0.0213 | train_acc: 0.9707 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 09 | train_loss: 5.7302 | val_loss: 0.0178 | train_acc: 0.9770 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 09 | train_loss: 5.7302 | val_loss: 0.0178 | train_acc: 0.9770 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 6.8274 | val_loss: 0.0166 | train_acc: 0.9700 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 10 | train_loss: 6.8274 | val_loss: 0.0166 | train_acc: 0.9700 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Epoch 11 | train_loss: 6.4199 | val_loss: 0.0251 | train_acc: 0.9730 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "Epoch 11 | train_loss: 6.4199 | val_loss: 0.0251 | train_acc: 0.9730 | val_acc: 1.0000 | val_F1: 1.0000\n",
      "Early stopping activado.\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_153554_img224_bs32_lr0.0001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=16 | lr=0.001 ===\n",
      "-> Test F1: 1.0000 | Val F1 (best): 1.0000 | modelo guardado: ../models/orFold2/mobilenet_pretrained_20251209_153554_img224_bs32_lr0.0001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=16 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 202.2798 | val_loss: 3.4093 | train_acc: 0.4190 | val_acc: 0.7551 | val_F1: 0.6907\n",
      "Epoch 01 | train_loss: 202.2798 | val_loss: 3.4093 | train_acc: 0.4190 | val_acc: 0.7551 | val_F1: 0.6907\n",
      "Epoch 02 | train_loss: 178.9855 | val_loss: 3.1548 | train_acc: 0.5233 | val_acc: 0.5102 | val_F1: 0.5240\n",
      "Epoch 02 | train_loss: 178.9855 | val_loss: 3.1548 | train_acc: 0.5233 | val_acc: 0.5102 | val_F1: 0.5240\n",
      "Epoch 03 | train_loss: 156.9795 | val_loss: 3.0793 | train_acc: 0.6073 | val_acc: 0.3673 | val_F1: 0.3143\n",
      "Epoch 03 | train_loss: 156.9795 | val_loss: 3.0793 | train_acc: 0.6073 | val_acc: 0.3673 | val_F1: 0.3143\n",
      "Epoch 04 | train_loss: 135.7129 | val_loss: 2.3289 | train_acc: 0.6783 | val_acc: 0.5510 | val_F1: 0.5766\n",
      "Epoch 04 | train_loss: 135.7129 | val_loss: 2.3289 | train_acc: 0.6783 | val_acc: 0.5510 | val_F1: 0.5766\n",
      "Epoch 05 | train_loss: 119.8447 | val_loss: 2.4670 | train_acc: 0.7237 | val_acc: 0.6939 | val_F1: 0.6993\n",
      "Epoch 05 | train_loss: 119.8447 | val_loss: 2.4670 | train_acc: 0.7237 | val_acc: 0.6939 | val_F1: 0.6993\n",
      "Epoch 06 | train_loss: 107.9748 | val_loss: 2.0431 | train_acc: 0.7700 | val_acc: 0.5714 | val_F1: 0.5402\n",
      "Epoch 06 | train_loss: 107.9748 | val_loss: 2.0431 | train_acc: 0.7700 | val_acc: 0.5714 | val_F1: 0.5402\n",
      "Epoch 07 | train_loss: 106.7410 | val_loss: 1.9934 | train_acc: 0.7647 | val_acc: 0.6939 | val_F1: 0.7034\n",
      "Epoch 07 | train_loss: 106.7410 | val_loss: 1.9934 | train_acc: 0.7647 | val_acc: 0.6939 | val_F1: 0.7034\n",
      "Epoch 08 | train_loss: 94.2765 | val_loss: 1.0215 | train_acc: 0.7930 | val_acc: 0.8571 | val_F1: 0.8630\n",
      "Epoch 08 | train_loss: 94.2765 | val_loss: 1.0215 | train_acc: 0.7930 | val_acc: 0.8571 | val_F1: 0.8630\n",
      "Epoch 09 | train_loss: 92.3696 | val_loss: 0.8300 | train_acc: 0.7997 | val_acc: 0.8776 | val_F1: 0.8819\n",
      "Epoch 09 | train_loss: 92.3696 | val_loss: 0.8300 | train_acc: 0.7997 | val_acc: 0.8776 | val_F1: 0.8819\n",
      "Epoch 10 | train_loss: 86.1281 | val_loss: 0.5418 | train_acc: 0.8083 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 10 | train_loss: 86.1281 | val_loss: 0.5418 | train_acc: 0.8083 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 11 | train_loss: 82.3279 | val_loss: 0.5757 | train_acc: 0.8093 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 11 | train_loss: 82.3279 | val_loss: 0.5757 | train_acc: 0.8093 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 12 | train_loss: 83.4679 | val_loss: 0.5748 | train_acc: 0.8117 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 12 | train_loss: 83.4679 | val_loss: 0.5748 | train_acc: 0.8117 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 86.0872 | val_loss: 0.8582 | train_acc: 0.8087 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 13 | train_loss: 86.0872 | val_loss: 0.8582 | train_acc: 0.8087 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 14 | train_loss: 79.8742 | val_loss: 0.6726 | train_acc: 0.8233 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 14 | train_loss: 79.8742 | val_loss: 0.6726 | train_acc: 0.8233 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 15 | train_loss: 78.7427 | val_loss: 0.4063 | train_acc: 0.8177 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 78.7427 | val_loss: 0.4063 | train_acc: 0.8177 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 16 | train_loss: 74.4825 | val_loss: 0.5053 | train_acc: 0.8340 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 16 | train_loss: 74.4825 | val_loss: 0.5053 | train_acc: 0.8340 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 17 | train_loss: 70.1558 | val_loss: 0.5115 | train_acc: 0.8450 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 17 | train_loss: 70.1558 | val_loss: 0.5115 | train_acc: 0.8450 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 18 | train_loss: 68.2543 | val_loss: 2.8115 | train_acc: 0.8490 | val_acc: 0.8571 | val_F1: 0.8481\n",
      "Epoch 18 | train_loss: 68.2543 | val_loss: 2.8115 | train_acc: 0.8490 | val_acc: 0.8571 | val_F1: 0.8481\n",
      "Epoch 19 | train_loss: 72.4647 | val_loss: 0.4819 | train_acc: 0.8370 | val_acc: 0.9388 | val_F1: 0.9395\n",
      "Epoch 19 | train_loss: 72.4647 | val_loss: 0.4819 | train_acc: 0.8370 | val_acc: 0.9388 | val_F1: 0.9395\n",
      "Epoch 20 | train_loss: 65.7447 | val_loss: 0.4581 | train_acc: 0.8500 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 20 | train_loss: 65.7447 | val_loss: 0.4581 | train_acc: 0.8500 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 21 | train_loss: 64.6147 | val_loss: 0.4837 | train_acc: 0.8527 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 21 | train_loss: 64.6147 | val_loss: 0.4837 | train_acc: 0.8527 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 22 | train_loss: 62.3927 | val_loss: 0.5054 | train_acc: 0.8607 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 22 | train_loss: 62.3927 | val_loss: 0.5054 | train_acc: 0.8607 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 23 | train_loss: 62.6604 | val_loss: 0.4488 | train_acc: 0.8603 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 23 | train_loss: 62.6604 | val_loss: 0.4488 | train_acc: 0.8603 | val_acc: 0.9388 | val_F1: 0.9359\n",
      "Epoch 24 | train_loss: 63.8749 | val_loss: 0.4956 | train_acc: 0.8663 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 24 | train_loss: 63.8749 | val_loss: 0.4956 | train_acc: 0.8663 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 25 | train_loss: 60.2586 | val_loss: 0.4869 | train_acc: 0.8633 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 25 | train_loss: 60.2586 | val_loss: 0.4869 | train_acc: 0.8633 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 26 | train_loss: 57.6830 | val_loss: 0.4054 | train_acc: 0.8753 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 26 | train_loss: 57.6830 | val_loss: 0.4054 | train_acc: 0.8753 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 27 | train_loss: 60.3684 | val_loss: 0.4914 | train_acc: 0.8607 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Epoch 27 | train_loss: 60.3684 | val_loss: 0.4914 | train_acc: 0.8607 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Epoch 28 | train_loss: 55.7245 | val_loss: 0.4618 | train_acc: 0.8757 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 28 | train_loss: 55.7245 | val_loss: 0.4618 | train_acc: 0.8757 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 29 | train_loss: 56.0350 | val_loss: 0.3520 | train_acc: 0.8687 | val_acc: 0.9388 | val_F1: 0.9395\n",
      "Epoch 29 | train_loss: 56.0350 | val_loss: 0.3520 | train_acc: 0.8687 | val_acc: 0.9388 | val_F1: 0.9395\n",
      "Epoch 30 | train_loss: 59.6500 | val_loss: 0.6726 | train_acc: 0.8740 | val_acc: 0.8980 | val_F1: 0.9009\n",
      "Epoch 30 | train_loss: 59.6500 | val_loss: 0.6726 | train_acc: 0.8740 | val_acc: 0.8980 | val_F1: 0.9009\n",
      "Epoch 31 | train_loss: 55.4503 | val_loss: 0.4294 | train_acc: 0.8733 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 31 | train_loss: 55.4503 | val_loss: 0.4294 | train_acc: 0.8733 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 32 | train_loss: 56.0422 | val_loss: 0.3861 | train_acc: 0.8697 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 32 | train_loss: 56.0422 | val_loss: 0.3861 | train_acc: 0.8697 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 33 | train_loss: 57.2701 | val_loss: 0.8357 | train_acc: 0.8767 | val_acc: 0.8571 | val_F1: 0.8630\n",
      "Epoch 33 | train_loss: 57.2701 | val_loss: 0.8357 | train_acc: 0.8767 | val_acc: 0.8571 | val_F1: 0.8630\n",
      "Epoch 34 | train_loss: 54.7443 | val_loss: 0.6267 | train_acc: 0.8790 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Early stopping activado.\n",
      "Epoch 34 | train_loss: 54.7443 | val_loss: 0.6267 | train_acc: 0.8790 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9624 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_155928_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "-> Test F1: 0.9624 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_155928_img224_bs16_lr0.001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=16 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 196.7047 | val_loss: 3.0654 | train_acc: 0.4470 | val_acc: 0.7755 | val_F1: 0.7596\n",
      "Epoch 01 | train_loss: 196.7047 | val_loss: 3.0654 | train_acc: 0.4470 | val_acc: 0.7755 | val_F1: 0.7596\n",
      "Epoch 02 | train_loss: 153.9668 | val_loss: 2.6917 | train_acc: 0.6297 | val_acc: 0.5714 | val_F1: 0.5338\n",
      "Epoch 02 | train_loss: 153.9668 | val_loss: 2.6917 | train_acc: 0.6297 | val_acc: 0.5714 | val_F1: 0.5338\n",
      "Epoch 03 | train_loss: 134.4324 | val_loss: 1.5281 | train_acc: 0.6923 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 03 | train_loss: 134.4324 | val_loss: 1.5281 | train_acc: 0.6923 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 04 | train_loss: 125.7842 | val_loss: 1.6837 | train_acc: 0.7240 | val_acc: 0.7347 | val_F1: 0.7334\n",
      "Epoch 04 | train_loss: 125.7842 | val_loss: 1.6837 | train_acc: 0.7240 | val_acc: 0.7347 | val_F1: 0.7334\n",
      "Epoch 05 | train_loss: 120.5280 | val_loss: 0.7912 | train_acc: 0.7203 | val_acc: 0.8980 | val_F1: 0.8929\n",
      "Epoch 05 | train_loss: 120.5280 | val_loss: 0.7912 | train_acc: 0.7203 | val_acc: 0.8980 | val_F1: 0.8929\n",
      "Epoch 06 | train_loss: 110.0827 | val_loss: 0.6733 | train_acc: 0.7563 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 06 | train_loss: 110.0827 | val_loss: 0.6733 | train_acc: 0.7563 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 07 | train_loss: 102.2407 | val_loss: 0.7183 | train_acc: 0.7667 | val_acc: 0.9388 | val_F1: 0.9376\n",
      "Epoch 07 | train_loss: 102.2407 | val_loss: 0.7183 | train_acc: 0.7667 | val_acc: 0.9388 | val_F1: 0.9376\n",
      "Epoch 08 | train_loss: 100.3480 | val_loss: 0.4578 | train_acc: 0.7657 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 08 | train_loss: 100.3480 | val_loss: 0.4578 | train_acc: 0.7657 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 09 | train_loss: 94.8019 | val_loss: 0.4664 | train_acc: 0.7880 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 09 | train_loss: 94.8019 | val_loss: 0.4664 | train_acc: 0.7880 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 10 | train_loss: 93.3471 | val_loss: 0.5048 | train_acc: 0.7890 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Epoch 10 | train_loss: 93.3471 | val_loss: 0.5048 | train_acc: 0.7890 | val_acc: 0.9388 | val_F1: 0.9379\n",
      "Epoch 11 | train_loss: 87.6907 | val_loss: 0.6143 | train_acc: 0.8050 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 11 | train_loss: 87.6907 | val_loss: 0.6143 | train_acc: 0.8050 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 12 | train_loss: 83.9847 | val_loss: 0.5489 | train_acc: 0.8183 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 12 | train_loss: 83.9847 | val_loss: 0.5489 | train_acc: 0.8183 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 82.9290 | val_loss: 0.5219 | train_acc: 0.8170 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 82.9290 | val_loss: 0.5219 | train_acc: 0.8170 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 79.0814 | val_loss: 0.3729 | train_acc: 0.8320 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 79.0814 | val_loss: 0.3729 | train_acc: 0.8320 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 76.6935 | val_loss: 0.4144 | train_acc: 0.8317 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 76.6935 | val_loss: 0.4144 | train_acc: 0.8317 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 16 | train_loss: 76.0991 | val_loss: 0.3851 | train_acc: 0.8290 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 16 | train_loss: 76.0991 | val_loss: 0.3851 | train_acc: 0.8290 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 17 | train_loss: 71.8393 | val_loss: 0.3933 | train_acc: 0.8453 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 17 | train_loss: 71.8393 | val_loss: 0.3933 | train_acc: 0.8453 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 18 | train_loss: 69.2812 | val_loss: 0.4393 | train_acc: 0.8480 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 18 | train_loss: 69.2812 | val_loss: 0.4393 | train_acc: 0.8480 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 19 | train_loss: 72.2476 | val_loss: 0.3621 | train_acc: 0.8437 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Early stopping activado.\n",
      "Epoch 19 | train_loss: 72.2476 | val_loss: 0.3621 | train_acc: 0.8437 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9805 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_161555_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "-> Test F1: 0.9805 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_161555_img224_bs16_lr0.0001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=32 | lr=0.001 ===\n",
      "Epoch 01 | train_loss: 96.5260 | val_loss: 1.6055 | train_acc: 0.4730 | val_acc: 0.5306 | val_F1: 0.5753\n",
      "Epoch 01 | train_loss: 96.5260 | val_loss: 1.6055 | train_acc: 0.4730 | val_acc: 0.5306 | val_F1: 0.5753\n",
      "Epoch 02 | train_loss: 79.2088 | val_loss: 1.7066 | train_acc: 0.6117 | val_acc: 0.5510 | val_F1: 0.5614\n",
      "Epoch 02 | train_loss: 79.2088 | val_loss: 1.7066 | train_acc: 0.6117 | val_acc: 0.5510 | val_F1: 0.5614\n",
      "Epoch 03 | train_loss: 68.4403 | val_loss: 1.3505 | train_acc: 0.6880 | val_acc: 0.6735 | val_F1: 0.6698\n",
      "Epoch 03 | train_loss: 68.4403 | val_loss: 1.3505 | train_acc: 0.6880 | val_acc: 0.6735 | val_F1: 0.6698\n",
      "Epoch 04 | train_loss: 59.1675 | val_loss: 0.7789 | train_acc: 0.7247 | val_acc: 0.8980 | val_F1: 0.8983\n",
      "Epoch 04 | train_loss: 59.1675 | val_loss: 0.7789 | train_acc: 0.7247 | val_acc: 0.8980 | val_F1: 0.8983\n",
      "Epoch 05 | train_loss: 54.7755 | val_loss: 1.0475 | train_acc: 0.7470 | val_acc: 0.8367 | val_F1: 0.8384\n",
      "Epoch 05 | train_loss: 54.7755 | val_loss: 1.0475 | train_acc: 0.7470 | val_acc: 0.8367 | val_F1: 0.8384\n",
      "Epoch 06 | train_loss: 49.3786 | val_loss: 0.4825 | train_acc: 0.7827 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 06 | train_loss: 49.3786 | val_loss: 0.4825 | train_acc: 0.7827 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 07 | train_loss: 45.1049 | val_loss: 0.3922 | train_acc: 0.8017 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 07 | train_loss: 45.1049 | val_loss: 0.3922 | train_acc: 0.8017 | val_acc: 0.9184 | val_F1: 0.9184\n",
      "Epoch 08 | train_loss: 44.0124 | val_loss: 0.3014 | train_acc: 0.8033 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 08 | train_loss: 44.0124 | val_loss: 0.3014 | train_acc: 0.8033 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 09 | train_loss: 41.6433 | val_loss: 0.3060 | train_acc: 0.8103 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 09 | train_loss: 41.6433 | val_loss: 0.3060 | train_acc: 0.8103 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 10 | train_loss: 40.5197 | val_loss: 0.3694 | train_acc: 0.8193 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 10 | train_loss: 40.5197 | val_loss: 0.3694 | train_acc: 0.8193 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 11 | train_loss: 37.2018 | val_loss: 0.2899 | train_acc: 0.8400 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 11 | train_loss: 37.2018 | val_loss: 0.2899 | train_acc: 0.8400 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 12 | train_loss: 36.9023 | val_loss: 0.5047 | train_acc: 0.8357 | val_acc: 0.9184 | val_F1: 0.9191\n",
      "Epoch 12 | train_loss: 36.9023 | val_loss: 0.5047 | train_acc: 0.8357 | val_acc: 0.9184 | val_F1: 0.9191\n",
      "Epoch 13 | train_loss: 36.1385 | val_loss: 0.3365 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 36.1385 | val_loss: 0.3365 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 34.2246 | val_loss: 0.2163 | train_acc: 0.8503 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 34.2246 | val_loss: 0.2163 | train_acc: 0.8503 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 30.4817 | val_loss: 1.4709 | train_acc: 0.8617 | val_acc: 0.6122 | val_F1: 0.5969\n",
      "Epoch 15 | train_loss: 30.4817 | val_loss: 1.4709 | train_acc: 0.8617 | val_acc: 0.6122 | val_F1: 0.5969\n",
      "Epoch 16 | train_loss: 36.1775 | val_loss: 0.1953 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 16 | train_loss: 36.1775 | val_loss: 0.1953 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 17 | train_loss: 30.1960 | val_loss: 0.2109 | train_acc: 0.8620 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 17 | train_loss: 30.1960 | val_loss: 0.2109 | train_acc: 0.8620 | val_acc: 0.9592 | val_F1: 0.9592\n",
      "Epoch 18 | train_loss: 33.7843 | val_loss: 0.2030 | train_acc: 0.8523 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 18 | train_loss: 33.7843 | val_loss: 0.2030 | train_acc: 0.8523 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 19 | train_loss: 34.5190 | val_loss: 0.1559 | train_acc: 0.8447 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 19 | train_loss: 34.5190 | val_loss: 0.1559 | train_acc: 0.8447 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 20 | train_loss: 30.7843 | val_loss: 0.1680 | train_acc: 0.8597 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 20 | train_loss: 30.7843 | val_loss: 0.1680 | train_acc: 0.8597 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 21 | train_loss: 29.6819 | val_loss: 0.2895 | train_acc: 0.8747 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 21 | train_loss: 29.6819 | val_loss: 0.2895 | train_acc: 0.8747 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 22 | train_loss: 29.0119 | val_loss: 1.0291 | train_acc: 0.8633 | val_acc: 0.6939 | val_F1: 0.7017\n",
      "Epoch 22 | train_loss: 29.0119 | val_loss: 1.0291 | train_acc: 0.8633 | val_acc: 0.6939 | val_F1: 0.7017\n",
      "Epoch 23 | train_loss: 27.8605 | val_loss: 0.2686 | train_acc: 0.8737 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 23 | train_loss: 27.8605 | val_loss: 0.2686 | train_acc: 0.8737 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 24 | train_loss: 28.2437 | val_loss: 0.2460 | train_acc: 0.8690 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 24 | train_loss: 28.2437 | val_loss: 0.2460 | train_acc: 0.8690 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 25 | train_loss: 30.6623 | val_loss: 0.3891 | train_acc: 0.8597 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 25 | train_loss: 30.6623 | val_loss: 0.3891 | train_acc: 0.8597 | val_acc: 0.9184 | val_F1: 0.9201\n",
      "Epoch 26 | train_loss: 28.9595 | val_loss: 0.3588 | train_acc: 0.8747 | val_acc: 0.8776 | val_F1: 0.8795\n",
      "Epoch 26 | train_loss: 28.9595 | val_loss: 0.3588 | train_acc: 0.8747 | val_acc: 0.8776 | val_F1: 0.8795\n",
      "Epoch 27 | train_loss: 27.3543 | val_loss: 0.2020 | train_acc: 0.8827 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 27 | train_loss: 27.3543 | val_loss: 0.2020 | train_acc: 0.8827 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 28 | train_loss: 27.9797 | val_loss: 0.1572 | train_acc: 0.8723 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Early stopping activado.\n",
      "Epoch 28 | train_loss: 27.9797 | val_loss: 0.1572 | train_acc: 0.8723 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9624 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_165918_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "-> Test F1: 0.9624 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_165918_img224_bs32_lr0.001.pth\n",
      "\n",
      "=== Entrenando simplecnn (pretrained) | img=224 | bs=32 | lr=0.0001 ===\n",
      "Epoch 01 | train_loss: 98.9457 | val_loss: 1.8226 | train_acc: 0.4633 | val_acc: 0.7347 | val_F1: 0.7609\n",
      "Epoch 01 | train_loss: 98.9457 | val_loss: 1.8226 | train_acc: 0.4633 | val_acc: 0.7347 | val_F1: 0.7609\n",
      "Epoch 02 | train_loss: 77.1584 | val_loss: 1.6638 | train_acc: 0.6297 | val_acc: 0.7143 | val_F1: 0.7282\n",
      "Epoch 02 | train_loss: 77.1584 | val_loss: 1.6638 | train_acc: 0.6297 | val_acc: 0.7143 | val_F1: 0.7282\n",
      "Epoch 03 | train_loss: 68.8041 | val_loss: 1.3730 | train_acc: 0.6870 | val_acc: 0.7347 | val_F1: 0.7472\n",
      "Epoch 03 | train_loss: 68.8041 | val_loss: 1.3730 | train_acc: 0.6870 | val_acc: 0.7347 | val_F1: 0.7472\n",
      "Epoch 04 | train_loss: 64.1459 | val_loss: 0.9816 | train_acc: 0.7110 | val_acc: 0.8776 | val_F1: 0.8780\n",
      "Epoch 04 | train_loss: 64.1459 | val_loss: 0.9816 | train_acc: 0.7110 | val_acc: 0.8776 | val_F1: 0.8780\n",
      "Epoch 05 | train_loss: 61.5506 | val_loss: 0.8776 | train_acc: 0.7323 | val_acc: 0.8571 | val_F1: 0.8580\n",
      "Epoch 05 | train_loss: 61.5506 | val_loss: 0.8776 | train_acc: 0.7323 | val_acc: 0.8571 | val_F1: 0.8580\n",
      "Epoch 06 | train_loss: 58.6833 | val_loss: 0.5745 | train_acc: 0.7400 | val_acc: 0.9184 | val_F1: 0.9191\n",
      "Epoch 06 | train_loss: 58.6833 | val_loss: 0.5745 | train_acc: 0.7400 | val_acc: 0.9184 | val_F1: 0.9191\n",
      "Epoch 07 | train_loss: 55.4741 | val_loss: 0.5636 | train_acc: 0.7523 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 07 | train_loss: 55.4741 | val_loss: 0.5636 | train_acc: 0.7523 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 08 | train_loss: 54.8434 | val_loss: 0.5684 | train_acc: 0.7660 | val_acc: 0.8980 | val_F1: 0.8970\n",
      "Epoch 08 | train_loss: 54.8434 | val_loss: 0.5684 | train_acc: 0.7660 | val_acc: 0.8980 | val_F1: 0.8970\n",
      "Epoch 09 | train_loss: 52.0925 | val_loss: 0.4505 | train_acc: 0.7723 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 09 | train_loss: 52.0925 | val_loss: 0.4505 | train_acc: 0.7723 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 10 | train_loss: 48.6823 | val_loss: 0.6918 | train_acc: 0.7887 | val_acc: 0.7959 | val_F1: 0.8057\n",
      "Epoch 10 | train_loss: 48.6823 | val_loss: 0.6918 | train_acc: 0.7887 | val_acc: 0.7959 | val_F1: 0.8057\n",
      "Epoch 11 | train_loss: 47.1897 | val_loss: 0.5662 | train_acc: 0.7903 | val_acc: 0.9388 | val_F1: 0.9404\n",
      "Epoch 11 | train_loss: 47.1897 | val_loss: 0.5662 | train_acc: 0.7903 | val_acc: 0.9388 | val_F1: 0.9404\n",
      "Epoch 12 | train_loss: 44.7335 | val_loss: 0.2391 | train_acc: 0.8070 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 12 | train_loss: 44.7335 | val_loss: 0.2391 | train_acc: 0.8070 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 43.2896 | val_loss: 0.2260 | train_acc: 0.8110 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 13 | train_loss: 43.2896 | val_loss: 0.2260 | train_acc: 0.8110 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 41.6922 | val_loss: 0.2363 | train_acc: 0.8260 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 14 | train_loss: 41.6922 | val_loss: 0.2363 | train_acc: 0.8260 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 15 | train_loss: 40.8133 | val_loss: 0.2465 | train_acc: 0.8173 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 15 | train_loss: 40.8133 | val_loss: 0.2465 | train_acc: 0.8173 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 16 | train_loss: 39.1349 | val_loss: 0.2466 | train_acc: 0.8277 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 16 | train_loss: 39.1349 | val_loss: 0.2466 | train_acc: 0.8277 | val_acc: 0.9388 | val_F1: 0.9381\n",
      "Epoch 17 | train_loss: 39.7114 | val_loss: 0.5494 | train_acc: 0.8350 | val_acc: 0.9388 | val_F1: 0.9404\n",
      "Epoch 17 | train_loss: 39.7114 | val_loss: 0.5494 | train_acc: 0.8350 | val_acc: 0.9388 | val_F1: 0.9404\n",
      "Epoch 18 | train_loss: 39.8289 | val_loss: 0.3084 | train_acc: 0.8290 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 18 | train_loss: 39.8289 | val_loss: 0.3084 | train_acc: 0.8290 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 19 | train_loss: 37.1602 | val_loss: 0.2175 | train_acc: 0.8367 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 19 | train_loss: 37.1602 | val_loss: 0.2175 | train_acc: 0.8367 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 20 | train_loss: 36.2188 | val_loss: 0.1620 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 20 | train_loss: 36.2188 | val_loss: 0.1620 | train_acc: 0.8413 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 21 | train_loss: 36.4891 | val_loss: 0.1810 | train_acc: 0.8447 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 21 | train_loss: 36.4891 | val_loss: 0.1810 | train_acc: 0.8447 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 22 | train_loss: 34.1535 | val_loss: 0.1601 | train_acc: 0.8460 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 22 | train_loss: 34.1535 | val_loss: 0.1601 | train_acc: 0.8460 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 23 | train_loss: 33.5434 | val_loss: 0.1430 | train_acc: 0.8507 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 23 | train_loss: 33.5434 | val_loss: 0.1430 | train_acc: 0.8507 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 24 | train_loss: 33.5864 | val_loss: 0.1732 | train_acc: 0.8570 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 24 | train_loss: 33.5864 | val_loss: 0.1732 | train_acc: 0.8570 | val_acc: 0.9796 | val_F1: 0.9793\n",
      "Epoch 25 | train_loss: 36.5782 | val_loss: 0.1680 | train_acc: 0.8410 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 25 | train_loss: 36.5782 | val_loss: 0.1680 | train_acc: 0.8410 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 26 | train_loss: 32.7103 | val_loss: 0.1851 | train_acc: 0.8633 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 26 | train_loss: 32.7103 | val_loss: 0.1851 | train_acc: 0.8633 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 27 | train_loss: 31.7329 | val_loss: 0.1418 | train_acc: 0.8637 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 27 | train_loss: 31.7329 | val_loss: 0.1418 | train_acc: 0.8637 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Epoch 28 | train_loss: 30.3002 | val_loss: 0.1774 | train_acc: 0.8707 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Early stopping activado.\n",
      "Epoch 28 | train_loss: 30.3002 | val_loss: 0.1774 | train_acc: 0.8707 | val_acc: 0.9592 | val_F1: 0.9580\n",
      "Early stopping activado.\n",
      "-> Test F1: 0.9440 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_174323_img224_bs32_lr0.0001.pth\n",
      "\n",
      "Mejores configuraciones por arquitectura (pretrained, según ValF1Best):\n",
      "      Fold      Model  ImageSize  BatchSize     LR  ValF1Best    TestF1  \\\n",
      "0  orFold2   resnet50        224         16  0.001   1.000000  1.000000   \n",
      "4  orFold2  mobilenet        224         16  0.001   1.000000  0.980996   \n",
      "8  orFold2  simplecnn        224         16  0.001   0.979315  0.962377   \n",
      "\n",
      "         Timestamp                                          ModelPath  \n",
      "0  20251209_135451  ../models/orFold2/resnet50_pretrained_20251209...  \n",
      "4  20251209_152112  ../models/orFold2/mobilenet_pretrained_2025120...  \n",
      "8  20251209_155928  ../models/orFold2/simplecnn_pretrained_2025120...  \n",
      "-> Test F1: 0.9440 | Val F1 (best): 0.9793 | modelo guardado: ../models/orFold2/simplecnn_pretrained_20251209_174323_img224_bs32_lr0.0001.pth\n",
      "\n",
      "Mejores configuraciones por arquitectura (pretrained, según ValF1Best):\n",
      "      Fold      Model  ImageSize  BatchSize     LR  ValF1Best    TestF1  \\\n",
      "0  orFold2   resnet50        224         16  0.001   1.000000  1.000000   \n",
      "4  orFold2  mobilenet        224         16  0.001   1.000000  0.980996   \n",
      "8  orFold2  simplecnn        224         16  0.001   0.979315  0.962377   \n",
      "\n",
      "         Timestamp                                          ModelPath  \n",
      "0  20251209_135451  ../models/orFold2/resnet50_pretrained_20251209...  \n",
      "4  20251209_152112  ../models/orFold2/mobilenet_pretrained_2025120...  \n",
      "8  20251209_155928  ../models/orFold2/simplecnn_pretrained_2025120...  \n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Grid Search con pesos preentrenados (excepto SimpleCNN)\n",
    "# ==============================\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Configuración general\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# root_fold = \"../data/imgShiny/224px/orFold1\"\n",
    "foldName = \"orFold2\"\n",
    "root_fold = f\"../data/imgShiny/224px/{foldName}\"\n",
    "train_dir = os.path.join(root_fold, \"train_aug\")\n",
    "val_dir   = os.path.join(root_fold, \"val\")\n",
    "test_dir  = os.path.join(root_fold, \"test\")\n",
    "\n",
    "classes = ['presence-defect', 'presence-good', 'absence']\n",
    "num_classes = len(classes)\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "timestamp_global = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_out_pretrained = f\"../results/metrics_pretrained_{timestamp_global}_{foldName}.csv\"\n",
    "csv_out_curves = f\"../results/curves_pretrained_{timestamp_global}_{foldName}.csv\"\n",
    "\n",
    "def make_transforms(image_size):\n",
    "    train_tfms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_test_tfms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_tfms, val_test_tfms\n",
    "\n",
    "# Red neuronal simple para imágenes\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, image_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * (image_size // 8) * (image_size // 8), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# build_model con pesos preentrenados excepto simplecnn\n",
    "def build_model_pretrained(arch_name: str, num_classes: int, image_size=224, pretrained_models_cache=None):\n",
    "    if pretrained_models_cache is not None and arch_name in pretrained_models_cache:\n",
    "        # Usar modelo cacheado (sin la última capa)\n",
    "        net = pretrained_models_cache[arch_name]\n",
    "        # Reemplazar la última capa según arquitectura\n",
    "        if arch_name == 'mobilenet':\n",
    "            in_features = net.classifier[1].in_features\n",
    "            net.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        elif arch_name == 'vgg16':\n",
    "            in_features = net.classifier[6].in_features\n",
    "            net.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "        elif arch_name == 'resnet50':\n",
    "            in_features = net.fc.in_features\n",
    "            net.fc = nn.Linear(in_features, num_classes)\n",
    "        elif arch_name == 'vit':\n",
    "            in_features = net.heads.head.in_features\n",
    "            net.heads.head = nn.Linear(in_features, num_classes)\n",
    "        return net\n",
    "    # Si no está en cache, cargar y cachear\n",
    "    if arch_name == 'mobilenet':\n",
    "        net = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        if pretrained_models_cache is not None:\n",
    "            pretrained_models_cache[arch_name] = net\n",
    "        in_features = net.classifier[1].in_features\n",
    "        net.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    elif arch_name == 'vgg16':\n",
    "        net = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        if pretrained_models_cache is not None:\n",
    "            pretrained_models_cache[arch_name] = net\n",
    "        in_features = net.classifier[6].in_features\n",
    "        net.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "    elif arch_name == 'resnet50':\n",
    "        net = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        if pretrained_models_cache is not None:\n",
    "            pretrained_models_cache[arch_name] = net\n",
    "        in_features = net.fc.in_features\n",
    "        net.fc = nn.Linear(in_features, num_classes)\n",
    "    elif arch_name == 'vit':\n",
    "        net = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        if pretrained_models_cache is not None:\n",
    "            pretrained_models_cache[arch_name] = net\n",
    "        in_features = net.heads.head.in_features\n",
    "        net.heads.head = nn.Linear(in_features, num_classes)\n",
    "    elif arch_name == 'simplecnn':\n",
    "        net = SimpleCNN(num_classes, image_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Arquitectura no soportada: {arch_name}\")\n",
    "    return net\n",
    "\n",
    "def evaluate_on_loader(model, loader, device=device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy().tolist())\n",
    "            y_pred.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(f\"[WARN] Longitudes distintas en evaluación: y_true={len(y_true)} vs y_pred={len(y_pred)}. Ajustando a longitud mínima.\")\n",
    "        m = min(len(y_true), len(y_pred))\n",
    "        y_true = y_true[:m]\n",
    "        y_pred = y_pred[:m]\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=200, patience=10, device=device):\n",
    "    best_f1 = -1.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Para guardar curvas\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_prec': [],\n",
    "        'val_prec': [],\n",
    "        'train_rec': [],\n",
    "        'val_rec': [],\n",
    "        'train_f1': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy().tolist())\n",
    "            train_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        # Métricas de train\n",
    "        if len(train_labels) != len(train_preds):\n",
    "            m = min(len(train_labels), len(train_preds))\n",
    "            train_labels = train_labels[:m]\n",
    "            train_preds = train_preds[:m]\n",
    "        train_acc  = accuracy_score(train_labels, train_preds)\n",
    "        train_prec = precision_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "        train_rec  = recall_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "        train_f1   = f1_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy().tolist())\n",
    "                val_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        if len(val_labels) != len(val_preds):\n",
    "            print(f\"[WARN] Longitudes distintas en validación: y_true={len(val_labels)} vs y_pred={len(val_preds)}. Ajustando a longitud mínima.\")\n",
    "            m = min(len(val_labels), len(val_preds))\n",
    "            val_labels = val_labels[:m]\n",
    "            val_preds  = val_preds[:m]\n",
    "\n",
    "        val_acc  = accuracy_score(val_labels, val_preds)\n",
    "        val_prec = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_rec  = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1   = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        # Guardar en historia\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(running_loss)\n",
    "        history['val_loss'].append(val_running_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_prec'].append(train_prec)\n",
    "        history['val_prec'].append(val_prec)\n",
    "        history['train_rec'].append(train_rec)\n",
    "        history['val_rec'].append(val_rec)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss: {running_loss:.4f} | val_loss: {val_running_loss:.4f} | train_acc: {train_acc:.4f} | val_acc: {val_acc:.4f} | val_F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "\n",
    "    # Extraer métricas del mejor epoch\n",
    "    idx = best_epoch - 1 if best_epoch > 0 else -1\n",
    "    best_metrics = {\n",
    "        'train_acc': history['train_acc'][idx],\n",
    "        'val_acc': history['val_acc'][idx],\n",
    "        'train_prec': history['train_prec'][idx],\n",
    "        'val_prec': history['val_prec'][idx],\n",
    "        'train_rec': history['train_rec'][idx],\n",
    "        'val_rec': history['val_rec'][idx],\n",
    "        'train_f1': history['train_f1'][idx],\n",
    "        'val_f1': history['val_f1'][idx],\n",
    "        'train_loss': history['train_loss'][idx],\n",
    "        'val_loss': history['val_loss'][idx]\n",
    "    }\n",
    "\n",
    "    return model, best_f1, best_epoch, history, best_metrics\n",
    "\n",
    "image_sizes   = [224]\n",
    "batch_sizes   = [16, 32]\n",
    "learning_rates= [1e-3, 1e-4]\n",
    "# learning_rates= [5e-3]\n",
    "epochs        = 200\n",
    "patience      = 10\n",
    "\n",
    "architectures = ['resnet50', 'mobilenet', 'simplecnn']\n",
    "# architectures = ['vit', 'simplecnn']\n",
    "# architectures = ['resnet50', 'mobilenet', 'vgg16', 'vit', 'simplecnn']\n",
    "# architectures = ['simplecnn']\n",
    "\n",
    "all_results = []\n",
    "all_curves = []\n",
    "fold_name = os.path.basename(root_fold.rstrip('/\\\\'))\n",
    "\n",
    "# --- Cache de modelos preentrenados para evitar descargas repetidas ---\n",
    "pretrained_models_cache = {}\n",
    "\n",
    "for arch in architectures:\n",
    "    for img_size in image_sizes:\n",
    "        train_tfms, val_test_tfms = make_transforms(img_size)\n",
    "\n",
    "        train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "        val_ds   = datasets.ImageFolder(val_dir,   transform=val_test_tfms)\n",
    "        test_ds  = datasets.ImageFolder(test_dir,  transform=val_test_tfms)\n",
    "        # model = build_model_pretrained(arch, num_classes, image_size=img_size).to(device)\n",
    "\n",
    "        for bs in batch_sizes:\n",
    "            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "            val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
    "            test_loader  = DataLoader(test_ds,  batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\n=== Entrenando {arch} (pretrained) | img={img_size} | bs={bs} | lr={lr} ===\")\n",
    "                start_t = time.time()\n",
    "\n",
    "                model = build_model_pretrained(arch, num_classes, image_size=img_size, pretrained_models_cache=pretrained_models_cache).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                model, best_val_f1, best_epoch, history, best_metrics = train_model(\n",
    "                    model,\n",
    "                    nn.CrossEntropyLoss(),\n",
    "                    optimizer,\n",
    "                    train_loader, val_loader,\n",
    "                    epochs=epochs, patience=patience, device=device\n",
    "                )\n",
    "                train_time = time.time() - start_t\n",
    "\n",
    "                acc, prec, rec, f1 = evaluate_on_loader(model, test_loader, device=device)\n",
    "\n",
    "                ts_conf = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "                model_path = f\"../models/{foldName}/{arch}_pretrained_{ts_conf}_img{img_size}_bs{bs}_lr{lr}.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                row = {\n",
    "                    \"Fold\": fold_name,\n",
    "                    \"Model\": arch,\n",
    "                    \"ImageSize\": img_size,\n",
    "                    \"BatchSize\": bs,\n",
    "                    \"LR\": lr,\n",
    "                    \"EpochsUsed\": best_epoch,\n",
    "                    \"Patience\": patience,\n",
    "                    \"ValF1Best\": best_val_f1,\n",
    "                    \"TrainAccuracy\": best_metrics['train_acc'],\n",
    "                    \"ValAccuracy\": best_metrics['val_acc'],\n",
    "                    \"TrainPrecision\": best_metrics['train_prec'],\n",
    "                    \"ValPrecision\": best_metrics['val_prec'],\n",
    "                    \"TrainRecall\": best_metrics['train_rec'],\n",
    "                    \"ValRecall\": best_metrics['val_rec'],\n",
    "                    \"TrainF1\": best_metrics['train_f1'],\n",
    "                    \"ValF1\": best_metrics['val_f1'],\n",
    "                    \"TrainLoss\": best_metrics['train_loss'],\n",
    "                    \"ValLoss\": best_metrics['val_loss'],\n",
    "                    \"TestAccuracy\": acc,\n",
    "                    \"TestPrecision\": prec,\n",
    "                    \"TestRecall\": rec,\n",
    "                    \"TestF1\": f1,\n",
    "                    \"TrainTimeSec\": round(train_time, 2),\n",
    "                    \"Timestamp\": ts_conf,\n",
    "                    \"ModelPath\": model_path\n",
    "                }\n",
    "                all_results.append(row)\n",
    "\n",
    "                # Guardar curvas para graficar\n",
    "                for i in range(len(history['epoch'])):\n",
    "                    curve_row = {\n",
    "                        \"Fold\": fold_name,\n",
    "                        \"Model\": arch,\n",
    "                        \"ImageSize\": img_size,\n",
    "                        \"BatchSize\": bs,\n",
    "                        \"LR\": lr,\n",
    "                        \"Epoch\": history['epoch'][i],\n",
    "                        \"TrainLoss\": history['train_loss'][i],\n",
    "                        \"ValLoss\": history['val_loss'][i],\n",
    "                        \"TrainAccuracy\": history['train_acc'][i],\n",
    "                        \"ValAccuracy\": history['val_acc'][i],\n",
    "                        \"TrainPrecision\": history['train_prec'][i],\n",
    "                        \"ValPrecision\": history['val_prec'][i],\n",
    "                        \"TrainRecall\": history['train_rec'][i],\n",
    "                        \"ValRecall\": history['val_rec'][i],\n",
    "                        \"TrainF1\": history['train_f1'][i],\n",
    "                        \"ValF1\": history['val_f1'][i],\n",
    "                        \"Timestamp\": ts_conf,\n",
    "                        \"ModelPath\": model_path\n",
    "                    }\n",
    "                    all_curves.append(curve_row)\n",
    "\n",
    "                df = pd.DataFrame(all_results)\n",
    "                df.to_csv(csv_out_pretrained, index=False)\n",
    "\n",
    "                df_curves = pd.DataFrame(all_curves)\n",
    "                df_curves.to_csv(csv_out_curves, index=False)\n",
    "\n",
    "                print(f\"-> Test F1: {f1:.4f} | Val F1 (best): {best_val_f1:.4f} | modelo guardado: {model_path}\")\n",
    "\n",
    "best_by_arch = df.sort_values(by=\"ValF1Best\", ascending=False).groupby(\"Model\").head(1)\n",
    "print(\"\\nMejores configuraciones por arquitectura (pretrained, según ValF1Best):\")\n",
    "print(best_by_arch[[\"Fold\",\"Model\",\"ImageSize\",\"BatchSize\",\"LR\",\"ValF1Best\",\"TestF1\",\"Timestamp\",\"ModelPath\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "=== Entrenando resnet50 (pretrained, png4ch) | img=224 | bs=16 | lr=0.001 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 343\u001b[39m\n\u001b[32m    341\u001b[39m model = build_model_pretrained(arch, num_classes, image_size=img_size, pretrained_models_cache=pretrained_models_cache).to(device)\n\u001b[32m    342\u001b[39m optimizer = optim.Adam(model.parameters(), lr=lr)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m model, best_val_f1, best_epoch, history, best_metrics = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m train_time = time.time() - start_t\n\u001b[32m    352\u001b[39m acc, prec, rec, f1 = evaluate_on_loader(model, test_loader, device=device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 218\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, train_loader, val_loader, epochs, patience, device)\u001b[39m\n\u001b[32m    215\u001b[39m loss.backward()\n\u001b[32m    216\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n\u001b[32m    220\u001b[39m train_preds.extend(preds.cpu().numpy().tolist())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Grid Search para png4ch con proyección 4→3 canales (1x1 conv)\n",
    "# ==============================\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "foldName = \"orFold2\"\n",
    "root_fold = f\"../data/png4ch/224px/{foldName}\"\n",
    "train_dir = os.path.join(root_fold, \"train_aug\")\n",
    "val_dir   = os.path.join(root_fold, \"val\")\n",
    "test_dir  = os.path.join(root_fold, \"test\")\n",
    "\n",
    "classes = ['absence', 'presence-defect', 'presence-good']\n",
    "num_classes = len(classes)\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "timestamp_global = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_out_pretrained = f\"../results/metrics_pretrained_png4ch_{timestamp_global}_{foldName}.csv\"\n",
    "csv_out_curves = f\"../results/curves_pretrained_png4ch_{timestamp_global}_{foldName}.csv\"\n",
    "\n",
    "def pil_loader_rgba(path):\n",
    "    img = Image.open(path)\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr]*4, axis=-1)\n",
    "    if arr.shape[2] == 3:\n",
    "        # Si ya es RGB, agregar canal extra (dummy)\n",
    "        arr = np.concatenate([arr, np.zeros_like(arr[..., :1])], axis=-1)\n",
    "    elif arr.shape[2] == 1:\n",
    "        arr = np.concatenate([arr]*4, axis=-1)\n",
    "    elif arr.shape[2] != 4:\n",
    "        raise ValueError(f\"Imagen con {arr.shape[2]} canales no soportada\")\n",
    "    arr = arr.astype(np.float32) / 255.0\n",
    "    tensor = torch.from_numpy(arr).permute(2,0,1)\n",
    "    return tensor\n",
    "\n",
    "class ImageFolderRGBA(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.loader = pil_loader_rgba\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n",
    "\n",
    "def make_transforms(image_size):\n",
    "    def train_tfms(x):\n",
    "        x = transforms.RandomResizedCrop(image_size)(x)\n",
    "        x = transforms.RandomHorizontalFlip()(x)\n",
    "        x = transforms.RandomRotation(20)(x)\n",
    "        return x\n",
    "    def val_test_tfms(x):\n",
    "        x = transforms.Resize((image_size, image_size))(x)\n",
    "        return x\n",
    "    return train_tfms, val_test_tfms\n",
    "\n",
    "class FourToThreeConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(4, 3, kernel_size=1, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.proj.weight[:] = 0.25\n",
    "            self.proj.bias[:] = 0.0\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, image_size, in_channels=4):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * (image_size // 8) * (image_size // 8), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def build_model_pretrained(arch_name: str, num_classes: int, image_size=224, pretrained_models_cache=None):\n",
    "    if arch_name in ['mobilenet', 'vgg16', 'resnet50', 'vit']:\n",
    "        proj = FourToThreeConv()\n",
    "        if pretrained_models_cache is not None and arch_name in pretrained_models_cache:\n",
    "            backbone = pretrained_models_cache[arch_name]\n",
    "        else:\n",
    "            if arch_name == 'mobilenet':\n",
    "                backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "                in_features = backbone.classifier[1].in_features\n",
    "                backbone.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "            elif arch_name == 'vgg16':\n",
    "                backbone = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "                in_features = backbone.classifier[6].in_features\n",
    "                backbone.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "            elif arch_name == 'resnet50':\n",
    "                backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "                in_features = backbone.fc.in_features\n",
    "                backbone.fc = nn.Linear(in_features, num_classes)\n",
    "            elif arch_name == 'vit':\n",
    "                backbone = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "                in_features = backbone.heads.head.in_features\n",
    "                backbone.heads.head = nn.Linear(in_features, num_classes)\n",
    "            if pretrained_models_cache is not None:\n",
    "                pretrained_models_cache[arch_name] = backbone\n",
    "        net = nn.Sequential(proj, backbone)\n",
    "    elif arch_name == 'simplecnn':\n",
    "        net = SimpleCNN(num_classes, image_size, in_channels=4)\n",
    "    else:\n",
    "        raise ValueError(f\"Arquitectura no soportada: {arch_name}\")\n",
    "    return net\n",
    "\n",
    "def evaluate_on_loader(model, loader, device=device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy().tolist())\n",
    "            y_pred.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(f\"[WARN] Longitudes distintas en evaluación: y_true={len(y_true)} vs y_pred={len(y_pred)}. Ajustando a longitud mínima.\")\n",
    "        m = min(len(y_true), len(y_pred))\n",
    "        y_true = y_true[:m]\n",
    "        y_pred = y_pred[:m]\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=200, patience=10, device=device):\n",
    "    best_f1 = -1.0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_prec': [],\n",
    "        'val_prec': [],\n",
    "        'train_rec': [],\n",
    "        'val_rec': [],\n",
    "        'train_f1': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy().tolist())\n",
    "            train_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        if len(train_labels) != len(train_preds):\n",
    "            m = min(len(train_labels), len(train_preds))\n",
    "            train_labels = train_labels[:m]\n",
    "            train_preds = train_preds[:m]\n",
    "        train_acc  = accuracy_score(train_labels, train_preds)\n",
    "        train_prec = precision_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "        train_rec  = recall_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "        train_f1   = f1_score(train_labels, train_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy().tolist())\n",
    "                val_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        if len(val_labels) != len(val_preds):\n",
    "            print(f\"[WARN] Longitudes distintas en validación: y_true={len(val_labels)} vs y_pred={len(val_preds)}. Ajustando a longitud mínima.\")\n",
    "            m = min(len(val_labels), len(val_preds))\n",
    "            val_labels = val_labels[:m]\n",
    "            val_preds  = val_preds[:m]\n",
    "\n",
    "        val_acc  = accuracy_score(val_labels, val_preds)\n",
    "        val_prec = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_rec  = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        val_f1   = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(running_loss)\n",
    "        history['val_loss'].append(val_running_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_prec'].append(train_prec)\n",
    "        history['val_prec'].append(val_prec)\n",
    "        history['train_rec'].append(train_rec)\n",
    "        history['val_rec'].append(val_rec)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss: {running_loss:.4f} | val_loss: {val_running_loss:.4f} | train_acc: {train_acc:.4f} | val_acc: {val_acc:.4f} | val_F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "\n",
    "    idx = best_epoch - 1 if best_epoch > 0 else -1\n",
    "    best_metrics = {\n",
    "        'train_acc': history['train_acc'][idx],\n",
    "        'val_acc': history['val_acc'][idx],\n",
    "        'train_prec': history['train_prec'][idx],\n",
    "        'val_prec': history['val_prec'][idx],\n",
    "        'train_rec': history['train_rec'][idx],\n",
    "        'val_rec': history['val_rec'][idx],\n",
    "        'train_f1': history['train_f1'][idx],\n",
    "        'val_f1': history['val_f1'][idx],\n",
    "        'train_loss': history['train_loss'][idx],\n",
    "        'val_loss': history['val_loss'][idx]\n",
    "    }\n",
    "\n",
    "    return model, best_f1, best_epoch, history, best_metrics\n",
    "\n",
    "image_sizes   = [224]\n",
    "batch_sizes   = [16, 32]\n",
    "learning_rates= [1e-3, 1e-4]\n",
    "# learning_rates= [5e-3]\n",
    "epochs        = 200\n",
    "patience      = 10\n",
    "\n",
    "architectures = ['resnet50', 'mobilenet', 'simplecnn']\n",
    "# architectures = ['vit', 'simplecnn']\n",
    "# architectures = ['resnet50', 'mobilenet', 'vgg16', 'vit', 'simplecnn']\n",
    "# architectures = ['simplecnn']\n",
    "\n",
    "all_results = []\n",
    "all_curves = []\n",
    "fold_name = os.path.basename(root_fold.rstrip('/\\\\'))\n",
    "\n",
    "pretrained_models_cache = {}\n",
    "\n",
    "for arch in architectures:\n",
    "    for img_size in image_sizes:\n",
    "        train_tfms, val_test_tfms = make_transforms(img_size)\n",
    "\n",
    "        train_ds = ImageFolderRGBA(train_dir, transform=train_tfms)\n",
    "        val_ds   = ImageFolderRGBA(val_dir,   transform=val_test_tfms)\n",
    "        test_ds  = ImageFolderRGBA(test_dir,  transform=val_test_tfms)\n",
    "\n",
    "        for bs in batch_sizes:\n",
    "            train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "            val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
    "            test_loader  = DataLoader(test_ds,  batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\n=== Entrenando {arch} (pretrained, png4ch) | img={img_size} | bs={bs} | lr={lr} ===\")\n",
    "                start_t = time.time()\n",
    "\n",
    "                model = build_model_pretrained(arch, num_classes, image_size=img_size, pretrained_models_cache=pretrained_models_cache).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                model, best_val_f1, best_epoch, history, best_metrics = train_model(\n",
    "                    model,\n",
    "                    nn.CrossEntropyLoss(),\n",
    "                    optimizer,\n",
    "                    train_loader, val_loader,\n",
    "                    epochs=epochs, patience=patience, device=device\n",
    "                )\n",
    "                train_time = time.time() - start_t\n",
    "\n",
    "                acc, prec, rec, f1 = evaluate_on_loader(model, test_loader, device=device)\n",
    "\n",
    "                ts_conf = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "                model_path = f\"../models/{foldName}/{arch}_pretrained_png4ch_{ts_conf}_img{img_size}_bs{bs}_lr{lr}.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                row = {\n",
    "                    \"Fold\": fold_name,\n",
    "                    \"Model\": arch,\n",
    "                    \"ImageSize\": img_size,\n",
    "                    \"BatchSize\": bs,\n",
    "                    \"LR\": lr,\n",
    "                    \"EpochsUsed\": best_epoch,\n",
    "                    \"Patience\": patience,\n",
    "                    \"ValF1Best\": best_val_f1,\n",
    "                    \"TrainAccuracy\": best_metrics['train_acc'],\n",
    "                    \"ValAccuracy\": best_metrics['val_acc'],\n",
    "                    \"TrainPrecision\": best_metrics['train_prec'],\n",
    "                    \"ValPrecision\": best_metrics['val_prec'],\n",
    "                    \"TrainRecall\": best_metrics['train_rec'],\n",
    "                    \"ValRecall\": best_metrics['val_rec'],\n",
    "                    \"TrainF1\": best_metrics['train_f1'],\n",
    "                    \"ValF1\": best_metrics['val_f1'],\n",
    "                    \"TrainLoss\": best_metrics['train_loss'],\n",
    "                    \"ValLoss\": best_metrics['val_loss'],\n",
    "                    \"TestAccuracy\": acc,\n",
    "                    \"TestPrecision\": prec,\n",
    "                    \"TestRecall\": rec,\n",
    "                    \"TestF1\": f1,\n",
    "                    \"TrainTimeSec\": round(train_time, 2),\n",
    "                    \"Timestamp\": ts_conf,\n",
    "                    \"ModelPath\": model_path\n",
    "                }\n",
    "                all_results.append(row)\n",
    "\n",
    "                for i in range(len(history['epoch'])):\n",
    "                    curve_row = {\n",
    "                        \"Fold\": fold_name,\n",
    "                        \"Model\": arch,\n",
    "                        \"ImageSize\": img_size,\n",
    "                        \"BatchSize\": bs,\n",
    "                        \"LR\": lr,\n",
    "                        \"Epoch\": history['epoch'][i],\n",
    "                        \"TrainLoss\": history['train_loss'][i],\n",
    "                        \"ValLoss\": history['val_loss'][i],\n",
    "                        \"TrainAccuracy\": history['train_acc'][i],\n",
    "                        \"ValAccuracy\": history['val_acc'][i],\n",
    "                        \"TrainPrecision\": history['train_prec'][i],\n",
    "                        \"ValPrecision\": history['val_prec'][i],\n",
    "                        \"TrainRecall\": history['train_rec'][i],\n",
    "                        \"ValRecall\": history['val_rec'][i],\n",
    "                        \"TrainF1\": history['train_f1'][i],\n",
    "                        \"ValF1\": history['val_f1'][i],\n",
    "                        \"Timestamp\": ts_conf,\n",
    "                        \"ModelPath\": model_path\n",
    "                    }\n",
    "                    all_curves.append(curve_row)\n",
    "\n",
    "                df = pd.DataFrame(all_results)\n",
    "                df.to_csv(csv_out_pretrained, index=False)\n",
    "\n",
    "                df_curves = pd.DataFrame(all_curves)\n",
    "                df_curves.to_csv(csv_out_curves, index=False)\n",
    "\n",
    "                print(f\"-> Test F1: {f1:.4f} | Val F1 (best): {best_val_f1:.4f} | modelo guardado: {model_path}\")\n",
    "\n",
    "best_by_arch = df.sort_values(by=\"ValF1Best\", ascending=False).groupby(\"Model\").head(1)\n",
    "print(\"\\nMejores configuraciones por arquitectura (pretrained, png4ch, según ValF1Best):\")\n",
    "print(best_by_arch[[\"Fold\",\"Model\",\"ImageSize\",\"BatchSize\",\"LR\",\"ValF1Best\",\"TestF1\",\"Timestamp\",\"ModelPath\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177242b",
   "metadata": {},
   "source": [
    "# XAI report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ece573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando XAI (sin torchcam) para resnet50 (orFold1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 184.99it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 133.45it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 128.45it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 134.52it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 133.38it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 143.02it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.88it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.92it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 128.07it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 128.07it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 140.97it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 180.58it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 169.10it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 175.08it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 207.78it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 180.29it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando XAI (sin torchcam) para mobilenet (orFold1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 167.28it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 129.75it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 126.57it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 137.73it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.84it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 139.05it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 131.85it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 130.47it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 122.12it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 137.02it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 185.69it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 171.95it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 156.57it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 246.38it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 162.01it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo no soportado: simplecnn\n",
      "\n",
      "Aplicando XAI (sin torchcam) para vgg16 (orFold1)...\n",
      "\n",
      "Aplicando XAI (sin torchcam) para vgg16 (orFold1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 145.35it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 122.62it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 119.81it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.73it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 136.42it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.48it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 138.48it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 131.32it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 127.84it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 121.86it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 137.91it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 172.72it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 171.59it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 173.76it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 197.83it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 174.82it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizaciones XAI (sin torchcam) generadas y guardadas en '../results/XAI/'.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# XAI para imgShiny\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GradCAM_Pure:\n",
    "    def __init__(self, model, target_layer, use_pp=False):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.use_pp = use_pp\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_backward_hook(backward_hook))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, class_idx):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        score = output[0, class_idx]\n",
    "        score.backward(retain_graph=True)\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        if self.use_pp:\n",
    "            # GradCAM++ weights\n",
    "            grads = gradients\n",
    "            grads_power_2 = grads ** 2\n",
    "            grads_power_3 = grads ** 3\n",
    "            sum_activations = torch.sum(activations, dim=(2, 3), keepdim=True)\n",
    "            eps = 1e-8\n",
    "            alpha_num = grads_power_2\n",
    "            alpha_denom = 2 * grads_power_2 + sum_activations * grads_power_3\n",
    "            alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom) * eps)\n",
    "            alphas = alpha_num / alpha_denom\n",
    "            weights = torch.sum(alphas * torch.relu(grads), dim=(2, 3))\n",
    "        else:\n",
    "            # GradCAM weights\n",
    "            weights = torch.mean(gradients, dim=(2, 3))\n",
    "        cam = torch.zeros(activations.shape[2:], dtype=torch.float32).to(input_tensor.device)\n",
    "        for i, w in enumerate(weights[0]):\n",
    "            cam += w * activations[0, i, :, :]\n",
    "        cam = torch.relu(cam)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-8)\n",
    "        cam = cam.cpu().numpy()\n",
    "        self.model.zero_grad()\n",
    "        return cam\n",
    "\n",
    "# --- Función para superponer el mapa CAM sobre la imagen original ---\n",
    "def overlay_cam_on_image(img_pil, cam_map, alpha=0.5, colormap=plt.cm.jet):\n",
    "    cam_map = np.uint8(255 * cam_map)\n",
    "    cam_map = Image.fromarray(cam_map).resize(img_pil.size, resample=Image.BILINEAR)\n",
    "    cam_map = np.array(cam_map)\n",
    "    heatmap = colormap(cam_map / 255.0)[:, :, :3]\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    overlay = np.array(img_pil) * (1 - alpha) + heatmap * alpha\n",
    "    overlay = np.uint8(overlay)\n",
    "    return overlay\n",
    "\n",
    "# --- Cargar CSV de mejores modelos ---\n",
    "csv_name = \"../results/metrics_pretrained_imgShiny_20251203_143542_fold1.csv\"\n",
    "best_models = pd.read_csv(csv_name)\n",
    "best_configs = best_models.sort_values(by='TestF1', ascending=False).groupby('Model').head(1)\n",
    "\n",
    "timestamp_global = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs(f\"../results/datasetpaper/{timestamp_global}\", exist_ok=True)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "classes = ['absence', 'presence-defect', 'presence-good']\n",
    "\n",
    "def load_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    tensor = test_transform(img).unsqueeze(0).to(device)\n",
    "    return img, tensor\n",
    "\n",
    "# --- Seleccionar 5 imágenes por clase del test set para cada fold ---\n",
    "for idx, row in best_configs.iterrows():\n",
    "    model_name = row['Model']\n",
    "    timestamp = row['Timestamp'] if 'Timestamp' in row else row['ModelPath'].split('_')[1]\n",
    "    fold = row['Fold'] if 'Fold' in row else 'FoldUnknown'\n",
    "    model_path = row['ModelPath'] if 'ModelPath' in row else f\"../models/{model_name}_{timestamp}.pth\"\n",
    "    test_root = os.path.join(\"../data/imgShiny/224px\", fold, 'test')\n",
    "\n",
    "    # Cargar modelo\n",
    "    if model_name == 'mobilenet':\n",
    "        model = models.mobilenet_v2(pretrained=False)\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(classes))\n",
    "        target_layer = model.features[-1]\n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, len(classes))\n",
    "        target_layer = model.features[-1]\n",
    "    elif model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, len(classes))\n",
    "        target_layer = model.layer4[-1]\n",
    "    else:\n",
    "        print(f\"Modelo no soportado: {model_name}\")\n",
    "        continue\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nAplicando XAI para {model_name} ({fold})...\")\n",
    "\n",
    "    # Crear carpetas de salida\n",
    "    for method_name in ['GradCAM', 'GradCAMpp', 'LIME']:\n",
    "        os.makedirs(f\"../results/datasetpaper/{timestamp_global}/{model_name}_{fold}/{method_name}\", exist_ok=True)\n",
    "\n",
    "    # Seleccionar imágenes de test\n",
    "    sample_images = []\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(test_root, cls)\n",
    "        if not os.path.exists(cls_path):\n",
    "            continue\n",
    "        imgs = os.listdir(cls_path)[:5]\n",
    "        sample_images.extend([(os.path.join(cls_path, img), cls) for img in imgs])\n",
    "\n",
    "    # Aplicar XAI para cada imagen\n",
    "    for img_path, cls in sample_images:\n",
    "        img_pil, img_tensor = load_image(img_path)\n",
    "        target_class = classes.index(cls)\n",
    "\n",
    "        # Obtener predicción del modelo\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            pred_idx = torch.argmax(output, dim=1).item()\n",
    "            pred_class = classes[pred_idx]\n",
    "\n",
    "        # Construir nombre de archivo con clase real y predicha\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        name_suffix = f\"_real-{cls}_pred-{pred_class}.png\"\n",
    "\n",
    "        # Grad-CAM\n",
    "        gradcam = GradCAM_Pure(model, target_layer, use_pp=False)\n",
    "        cam_map = gradcam(img_tensor, target_class)\n",
    "        overlay = overlay_cam_on_image(img_pil, cam_map, alpha=0.5)\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../results/datasetpaper/{timestamp_global}/{model_name}_{fold}/GradCAM/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "        gradcam.remove_hooks()\n",
    "\n",
    "        # Grad-CAM++\n",
    "        gradcampp = GradCAM_Pure(model, target_layer, use_pp=True)\n",
    "        cam_map_pp = gradcampp(img_tensor, target_class)\n",
    "        overlay_pp = overlay_cam_on_image(img_pil, cam_map_pp, alpha=0.5)\n",
    "        plt.imshow(overlay_pp)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../results/datasetpaper/{timestamp_global}/{model_name}_{fold}/GradCAMpp/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "        gradcampp.remove_hooks()\n",
    "\n",
    "        # LIME\n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        img_np = np.array(img_pil)\n",
    "\n",
    "        def predict_fn(images):\n",
    "            batch = torch.stack([test_transform(Image.fromarray(img)).to(device) for img in images])\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch)\n",
    "            return outputs.cpu().numpy()\n",
    "\n",
    "        explanation = explainer.explain_instance(\n",
    "            img_np,\n",
    "            predict_fn,\n",
    "            top_labels=1,\n",
    "            hide_color=0,\n",
    "            num_samples=1000\n",
    "        )\n",
    "\n",
    "        # Obtener imagen y máscara\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            explanation.top_labels[0],\n",
    "            positive_only=True,\n",
    "            num_features=5,\n",
    "            hide_rest=False\n",
    "        )\n",
    "\n",
    "        # Superponer bordes de superpixeles\n",
    "        from scipy.ndimage import binary_dilation\n",
    "\n",
    "        # Aumentar el grosor de los bordes (ajusta iterations para más grosor)\n",
    "        dilated_mask = binary_dilation(mask, iterations=3)\n",
    "\n",
    "        # Aplicar transparencia del 30% a la imagen temp (convertir a float, escalar, y volver a uint8)\n",
    "        temp_alpha = temp.astype(float) * 0.7 + 255 * 0.3  # 70% imagen, 30% blanco para remarcar LIME\n",
    "        temp_alpha = np.clip(temp_alpha, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Mostrar con bordes gruesos y fondo atenuado\n",
    "        plt.imshow(mark_boundaries(temp_alpha, dilated_mask, color=(1, 0, 0), mode='thick'))\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Guardar resultado\n",
    "        plt.savefig(f\"../results/datasetpaper/{timestamp_global}/{model_name}_{fold}/LIME/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "\n",
    "        # # SHAP\n",
    "        # background = torch.cat([img_tensor for _ in range(5)], dim=0)\n",
    "        # e = shap.DeepExplainer(model, background)\n",
    "        # shap_values = e.shap_values(img_tensor)\n",
    "        # shap.image_plot(shap_values, np.array([np.array(img_pil)]), show=False)\n",
    "        # plt.savefig(f\"../results/XAI/{model_name}_{fold}/SHAP/{base_name}{name_suffix}\")\n",
    "        # plt.close()\n",
    "\n",
    "print(\"Visualizaciones XAI (sin torchcam) generadas y guardadas en '../results/XAI/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d315fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando XAI (sin torchcam) para mobilenet (orFold1, RGBA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 169.04it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 168.31it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 171.87it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 165.29it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 198.98it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 146.48it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 158.54it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 145.18it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 178.60it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 165.86it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 249.78it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 225.08it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 233.94it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 196.68it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 193.72it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando XAI (sin torchcam) para resnet50 (orFold1, RGBA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 121.31it/s]\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 121.78it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 113.02it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 113.06it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 145.67it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 107.03it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.95it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 108.05it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 126.04it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 122.91it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 168.90it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 172.69it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 178.68it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 144.44it/s]\n",
      "\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "c:\\Users\\nalex\\OneDrive\\Documentos\\PhD\\VSprojects\\BGA_defectClass\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1842: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 143.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo no soportado: simplecnn\n",
      "Visualizaciones XAI (RGBA, sin torchcam) generadas y guardadas en '../results/XAI_png4ch/'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# XAI Report for RGBA images (png4ch, 4 canales)\n",
    "# ============================================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GradCAM_Pure:\n",
    "    def __init__(self, model, target_layer, use_pp=False):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.use_pp = use_pp\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_backward_hook(backward_hook))\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "    def __call__(self, input_tensor, class_idx):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        score = output[0, class_idx]\n",
    "        score.backward(retain_graph=True)\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        if self.use_pp:\n",
    "            grads = gradients\n",
    "            grads_power_2 = grads ** 2\n",
    "            grads_power_3 = grads ** 3\n",
    "            sum_activations = torch.sum(activations, dim=(2, 3), keepdim=True)\n",
    "            eps = 1e-8\n",
    "            alpha_num = grads_power_2\n",
    "            alpha_denom = 2 * grads_power_2 + sum_activations * grads_power_3\n",
    "            alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom) * eps)\n",
    "            alphas = alpha_num / alpha_denom\n",
    "            weights = torch.sum(alphas * torch.relu(grads), dim=(2, 3))\n",
    "        else:\n",
    "            weights = torch.mean(gradients, dim=(2, 3))\n",
    "        cam = torch.zeros(activations.shape[2:], dtype=torch.float32).to(input_tensor.device)\n",
    "        for i, w in enumerate(weights[0]):\n",
    "            cam += w * activations[0, i, :, :]\n",
    "        cam = torch.relu(cam)\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-8)\n",
    "        cam = cam.cpu().numpy()\n",
    "        self.model.zero_grad()\n",
    "        return cam\n",
    "\n",
    "def overlay_cam_on_image(img_pil, cam_map, alpha=0.5, colormap=plt.cm.jet):\n",
    "    cam_map = np.uint8(255 * cam_map)\n",
    "    cam_map = Image.fromarray(cam_map).resize(img_pil.size, resample=Image.BILINEAR)\n",
    "    cam_map = np.array(cam_map)\n",
    "    heatmap = colormap(cam_map / 255.0)[:, :, :3]\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    overlay = np.array(img_pil)[..., :3] * (1 - alpha) + heatmap * alpha\n",
    "    overlay = np.uint8(overlay)\n",
    "    return overlay\n",
    "\n",
    "def pil_loader_rgba(path):\n",
    "    img = Image.open(path)\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr]*4, axis=-1)\n",
    "    if arr.shape[2] == 3:\n",
    "        arr = np.concatenate([arr, np.zeros_like(arr[..., :1])], axis=-1)\n",
    "    elif arr.shape[2] == 1:\n",
    "        arr = np.concatenate([arr]*4, axis=-1)\n",
    "    elif arr.shape[2] != 4:\n",
    "        raise ValueError(f\"Imagen con {arr.shape[2]} canales no soportada\")\n",
    "    arr = arr.astype(np.float32) / 255.0\n",
    "    tensor = torch.from_numpy(arr).permute(2,0,1)\n",
    "    return tensor\n",
    "\n",
    "csv_name = \"../results/metrics_pretrained_png4ch_20251203_233013_fold1.csv\"\n",
    "import glob\n",
    "csv_files = glob.glob(csv_name)\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No se encontró el archivo de métricas para png4ch\")\n",
    "csv_name = csv_files[-1]\n",
    "best_models = pd.read_csv(csv_name)\n",
    "best_configs = best_models.sort_values(by='TestF1', ascending=False).groupby('Model').head(1)\n",
    "\n",
    "timestamp_global = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs(f\"../results/dataset_png4ch/{timestamp_global}\", exist_ok=True)\n",
    "\n",
    "classes = ['absence', 'presence-defect', 'presence-good']\n",
    "\n",
    "def load_image_rgba(img_path):\n",
    "    tensor = pil_loader_rgba(img_path)\n",
    "    img = Image.open(img_path).convert('RGBA')\n",
    "    return img, tensor.unsqueeze(0).to(device)\n",
    "\n",
    "for idx, row in best_configs.iterrows():\n",
    "    model_name = row['Model']\n",
    "    timestamp = row['Timestamp'] if 'Timestamp' in row else row['ModelPath'].split('_')[1]\n",
    "    fold = row['Fold'] if 'Fold' in row else 'FoldUnknown'\n",
    "    model_path = row['ModelPath'] if 'ModelPath' in row else f\"../models/{model_name}_{timestamp}.pth\"\n",
    "    test_root = os.path.join(\"../data/png4ch/224px\", fold, 'test')\n",
    "\n",
    "    from torchvision.models import mobilenet_v2, vgg16, resnet50, vit_b_16\n",
    "    from torch import nn\n",
    "    if model_name == 'mobilenet':\n",
    "        proj = nn.Conv2d(4, 3, kernel_size=1, bias=True)\n",
    "        backbone = mobilenet_v2(pretrained=False)\n",
    "        backbone.classifier[1] = nn.Linear(backbone.classifier[1].in_features, len(classes))\n",
    "        target_layer = backbone.features[-1]\n",
    "        model = nn.Sequential(proj, backbone)\n",
    "    elif model_name == 'vgg16':\n",
    "        proj = nn.Conv2d(4, 3, kernel_size=1, bias=True)\n",
    "        backbone = vgg16(pretrained=False)\n",
    "        backbone.classifier[6] = nn.Linear(backbone.classifier[6].in_features, len(classes))\n",
    "        target_layer = backbone.features[-1]\n",
    "        model = nn.Sequential(proj, backbone)\n",
    "    elif model_name == 'resnet50':\n",
    "        proj = nn.Conv2d(4, 3, kernel_size=1, bias=True)\n",
    "        backbone = resnet50(pretrained=False)\n",
    "        backbone.fc = nn.Linear(backbone.fc.in_features, len(classes))\n",
    "        target_layer = backbone.layer4[-1]\n",
    "        model = nn.Sequential(proj, backbone)\n",
    "    else:\n",
    "        print(f\"Modelo no soportado: {model_name}\")\n",
    "        continue\n",
    "\n",
    "    # --- Cargar state_dict y mapear claves ---\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model_state = model.state_dict()\n",
    "    if set(state.keys()) != set(model_state.keys()):\n",
    "        new_state = {}\n",
    "        for k in state.keys():\n",
    "            if k.startswith('0.proj.'):\n",
    "                newk = k.replace('0.proj.', '0.')\n",
    "                new_state[newk] = state[k]\n",
    "            else:\n",
    "                new_state[k] = state[k]\n",
    "        state = new_state\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nAplicando XAI (sin torchcam) para {model_name} ({fold}, RGBA)...\")\n",
    "\n",
    "    for method_name in ['GradCAM', 'GradCAMpp', 'LIME']:\n",
    "        os.makedirs(f\"../results/dataset_png4ch/{timestamp_global}/{model_name}_{fold}/{method_name}\", exist_ok=True)\n",
    "\n",
    "    sample_images = []\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(test_root, cls)\n",
    "        if not os.path.exists(cls_path):\n",
    "            continue\n",
    "        imgs = os.listdir(cls_path)[:5]\n",
    "        sample_images.extend([(os.path.join(cls_path, img), cls) for img in imgs])\n",
    "\n",
    "    for img_path, cls in sample_images:\n",
    "        img_pil, img_tensor = load_image_rgba(img_path)\n",
    "        target_class = classes.index(cls)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            pred_idx = torch.argmax(output, dim=1).item()\n",
    "            pred_class = classes[pred_idx]\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        name_suffix = f\"_real-{cls}_pred-{pred_class}.png\"\n",
    "\n",
    "        gradcam = GradCAM_Pure(model, target_layer, use_pp=False)\n",
    "        cam_map = gradcam(img_tensor, target_class)\n",
    "        overlay = overlay_cam_on_image(img_pil, cam_map, alpha=0.5)\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../results/dataset_png4ch/{timestamp_global}/{model_name}_{fold}/GradCAM/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "        gradcam.remove_hooks()\n",
    "\n",
    "        gradcampp = GradCAM_Pure(model, target_layer, use_pp=True)\n",
    "        cam_map_pp = gradcampp(img_tensor, target_class)\n",
    "        overlay_pp = overlay_cam_on_image(img_pil, cam_map_pp, alpha=0.5)\n",
    "        plt.imshow(overlay_pp)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../results/dataset_png4ch/{timestamp_global}/{model_name}_{fold}/GradCAMpp/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "        gradcampp.remove_hooks()\n",
    "\n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        img_np = np.array(img_pil)[..., :3]\n",
    "        def predict_fn(images):\n",
    "            batch = torch.stack([torch.from_numpy(img.transpose(2,0,1)).float().div(255).unsqueeze(0)[0] for img in images]).to(device)\n",
    "            if batch.shape[1] == 3:\n",
    "                batch = torch.cat([batch, torch.zeros(batch.shape[0],1,batch.shape[2],batch.shape[3], device=batch.device)], dim=1)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch)\n",
    "            return outputs.cpu().numpy()\n",
    "\n",
    "        explanation = explainer.explain_instance(\n",
    "            img_np,\n",
    "            predict_fn,\n",
    "            top_labels=1,\n",
    "            hide_color=0,\n",
    "            num_samples=1000\n",
    "        )\n",
    "\n",
    "        temp, mask = explanation.get_image_and_mask(\n",
    "            explanation.top_labels[0],\n",
    "            positive_only=True,\n",
    "            num_features=5,\n",
    "            hide_rest=False\n",
    "        )\n",
    "\n",
    "        dilated_mask = binary_dilation(mask, iterations=3)\n",
    "        temp_alpha = temp.astype(float) * 0.7 + 255 * 0.3\n",
    "        temp_alpha = np.clip(temp_alpha, 0, 255).astype(np.uint8)\n",
    "\n",
    "        plt.imshow(mark_boundaries(temp_alpha, dilated_mask, color=(1, 0, 0), mode='thick'))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"../results/dataset_png4ch/{timestamp_global}/{model_name}_{fold}/LIME/{base_name}{name_suffix}\")\n",
    "        plt.close()\n",
    "\n",
    "print(\"Visualizaciones XAI (RGBA, sin torchcam) generadas y guardadas en '../results/XAI_png4ch/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee5ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absence: aug_0096.png -> RGBA (('R', 'G', 'B', 'A')) -> shape: (224, 224), canales: 4\n",
      "presence-defect: aug_0224.png -> RGBA (('R', 'G', 'B', 'A')) -> shape: (224, 224), canales: 4\n",
      "presence-good: aug_0239.png -> RGBA (('R', 'G', 'B', 'A')) -> shape: (224, 224), canales: 4\n"
     ]
    }
   ],
   "source": [
    "# --- Lectura de una imagen aleatoria por subcarpeta para identificar número de canales ---\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# base_dir = \"../data/png4ch/original_copia\"\n",
    "# base_dir = \"../data/png4ch/original\"\n",
    "base_dir = \"../data/png4ch/224px/orFold1/train_aug\"\n",
    "\n",
    "for subfolder in os.listdir(base_dir):\n",
    "    subfolder_path = os.path.join(base_dir, subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "    images = [f for f in os.listdir(subfolder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))]\n",
    "    if not images:\n",
    "        print(f\"No hay imágenes en {subfolder_path}\")\n",
    "        continue\n",
    "    img_file = random.choice(images)\n",
    "    img_path = os.path.join(subfolder_path, img_file)\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        print(f\"{subfolder}: {img_file} -> {img.mode} ({img.getbands()}) -> shape: {img.size}, canales: {len(img.getbands())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error abriendo {img_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
